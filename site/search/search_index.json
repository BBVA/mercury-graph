{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"mercury-graph","text":"<p><code>mercury-graph</code> is a Python library that offers graph analytics capabilities with a technology-agnostic API, enabling users to apply a curated range of performant and scalable algorithms and utilities regardless of the underlying data framework. The consistent, scikit-like interface abstracts away the complexities of internal transformations, allowing users to effortlessly switch between different graph representations to leverage optimized algorithms implemented using pure Python, numba, networkx and PySpark GraphFrames.</p> <p>Currently implemented submodules in <code>mercury.graph</code> include:</p> <ul> <li> <p><code>mercury.graph.core</code>, with the main classes of the library that create and store the graphs' data and properties.</p> </li> <li> <p><code>mercury.graph.ml</code>, with graph theory and machine learning algorithms such as Louvain community detection, spectral clustering, Markov chains, spreading activation-based diffusion models and graph random walkers.</p> </li> <li> <p><code>mercury.graph.embeddings</code>, with classes that calculate graph embeddings in different ways, such as following the Node2Vec algorithm.</p> </li> <li> <p><code>mercury.graph.viz</code>, with capabilities for graph visualization.</p> </li> </ul>"},{"location":"#repository","title":"Repository","text":"<p>The website for the GitHub repository can be found here.</p>"},{"location":"reference/core/","title":"mercury.graph.core","text":""},{"location":"reference/core/#mercury.graph.core.Graph","title":"<code>mercury.graph.core.Graph(data=None, keys=None, nodes=None)</code>","text":"<p>This is the main class in mercury.graph.</p> <p>This class seamlessly abstracts the underlying technology used to represent the graph. You can create a graph passing the following objects to the constructor:</p> <ul> <li>A pandas DataFrame containing edges (with a keys dictionary to specify the columns and possibly a nodes DataFrame)</li> <li>A pyspark DataFrame containing edges (with a keys dictionary to specify the columns and possibly a nodes DataFrame)</li> <li>A networkx graph</li> <li>A graphframes graph</li> </ul> <p>Bear in mind that the graph object is immutable. This means that you can't modify the graph object once it has been created. If you want to modify it, you have to create a new graph object.</p> <p>The graph object provides:</p> <ul> <li>Properties to access the graph in different formats (networkx, graphframes, dgl)</li> <li>Properties with metrics and summary information that are calculated on demand and technology independent.</li> <li>It is inherited by other graph classes in mercury-graph providing ML algorithms such as graph embedding, visualization, etc.</li> </ul> <p>Using this class from the other classes in mercury-graph:</p> <p>The other classes in mercury-graph define models or functionalities that are based on graphs. They use a Scikit-learn-like API to interact with the graph object. This means that the graph object is passed to the class constructor and the class follow the Scikit-learn conventions. It is recommended to follow the same conventions when creating your own classes to work with mercury-graph.</p> <p>The conventions can be found here:</p> <ul> <li>Scikit API</li> <li>On scikit conventions</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>(DataFrame, Graph or DataFrame)</code> <p>The data to create the graph from.  It can be a pandas DataFrame, a networkx Graph, a pyspark DataFrame, or a Graphframe.  In case it already contains a graph (networkx or graphframes), the keys and nodes arguments are ignored.</p> <code>None</code> <code>keys</code> <code>dict</code> <p>A dictionary with keys to specify the columns in the data DataFrame. The keys are:</p> <ul> <li>'src': The name of the column with the source node.</li> <li>'dst': The name of the column with the destination node.</li> <li>'id': The name of the column with the node id.</li> <li>'weight': The name of the column with the edge weight.</li> <li>'directed': A boolean to specify if the graph is directed. (Only for pyspark DataFrames)</li> </ul> <p>When the keys argument is not provided or the key is missing, the default values are:</p> <ul> <li>'src': 'src'</li> <li>'dst': 'dst'</li> <li>'id': 'id'</li> <li>'weight': 'weight'</li> <li>'directed': True</li> </ul> <code>None</code> <code>nodes</code> <code>DataFrame</code> <p>A pandas DataFrame or a pyspark DataFrame with the nodes data. (Only when <code>data</code> is pandas or pyspark DataFrame and with the same type as <code>data</code>) If not given, the nodes are inferred from the edges DataFrame.</p> <code>None</code> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def __init__(self, data = None, keys = None, nodes = None):\n    self._as_networkx = None\n    self._as_graphframe = None\n    self._as_dgl = None\n    self._degree = None\n    self._in_degree = None\n    self._out_degree = None\n    self._closeness_centrality = None\n    self._betweenness_centrality = None\n    self._pagerank = None\n    self._connected_components = None\n    self._nodes_colnames = None\n    self._edges_colnames = None\n\n    self._number_of_nodes = 0\n    self._number_of_edges = 0\n    self._node_ix = 0\n    self._is_directed = False\n    self._is_weighted = False\n\n    self._init_values = {k: v for k, v in locals().items() if k in inspect.signature(self.__init__).parameters}\n\n    if type(data) == pd.core.frame.DataFrame:\n        self._from_pandas(data, nodes, keys)\n        return\n\n    if isinstance(data, nx.Graph):      # This is the most general case, including: ...Graph, ...DiGraph and ...MultiGraph\n        self._from_networkx(data)\n        return\n\n    spark_int = SparkInterface()\n\n    if pyspark_installed and graphframes_installed:\n        if type(data) == spark_int.type_spark_dataframe:\n            self._from_dataframe(data, nodes, keys)\n            return\n\n        if type(data) == spark_int.type_graphframe:\n            self._from_graphframes(data)\n            return\n\n    raise ValueError('Invalid input data. (Expected: pandas DataFrame, a networkx Graph, a pyspark DataFrame, a graphframes Graph.)')\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph.betweenness_centrality","title":"<code>betweenness_centrality</code>  <code>property</code>","text":"<p>Returns the betweenness centrality of each node in the graph as a Python dictionary.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.closeness_centrality","title":"<code>closeness_centrality</code>  <code>property</code>","text":"<p>Returns the closeness centrality of each node in the graph as a Python dictionary.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.connected_components","title":"<code>connected_components</code>  <code>property</code>","text":"<p>Returns the connected components of each node in the graph as a Python dictionary.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.degree","title":"<code>degree</code>  <code>property</code>","text":"<p>Returns the degree of each node in the graph as a Python dictionary.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.dgl","title":"<code>dgl</code>  <code>property</code>","text":"<p>Returns the graph as a DGL graph.</p> <p>If the graph has not been converted to a DGL graph yet, it will be converted and cached for future use.</p> <p>Returns:</p> Type Description <code>DGLGraph</code> <p>The graph represented as a DGL graph.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.edges","title":"<code>edges</code>  <code>property</code>","text":"<p>Returns an iterator over the edges in the graph.</p> <p>Returns:</p> Type Description <code>EdgeIterator</code> <p>An iterator object that allows iterating over the edges in the graph.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.edges_colnames","title":"<code>edges_colnames</code>  <code>property</code>","text":"<p>Returns the column names of the edges DataFrame.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.graphframe","title":"<code>graphframe</code>  <code>property</code>","text":"<p>Returns the graph as a GraphFrame.</p> <p>If the graph has not been converted to a GraphFrame yet, it will be converted and cached for future use.</p> <p>Returns:</p> Type Description <code>GraphFrame</code> <p>The graph represented as a GraphFrame.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.in_degree","title":"<code>in_degree</code>  <code>property</code>","text":"<p>Returns the in-degree of each node in the graph as a Python dictionary.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.is_directed","title":"<code>is_directed</code>  <code>property</code>","text":"<p>Returns True if the graph is directed, False otherwise.</p> Note <p>Graphs created using graphframes are always directed. The way around it is to add the reverse edges to the graph. This can be done by creating the Graph with pyspark DataFrame() and defining a key 'directed' set as False in the <code>dict</code> argument. Otherwise, the graph will be considered directed even if these reversed edges have been created by other means this class cannot be aware of.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.is_weighted","title":"<code>is_weighted</code>  <code>property</code>","text":"<p>Returns True if the graph is weighted, False otherwise.</p> <p>A graph is considered weight if it has a column named 'weight' in the edges DataFrame or the column has a different name and that name is passed in the <code>dict</code> argument as the 'weight' key.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.networkx","title":"<code>networkx</code>  <code>property</code>","text":"<p>Returns the graph representation as a NetworkX graph.</p> <p>If the graph has not been converted to NetworkX format yet, it will be converted and cached for future use.</p> <p>Returns:</p> Type Description <code>Graph</code> <p>The graph representation as a NetworkX graph.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.nodes","title":"<code>nodes</code>  <code>property</code>","text":"<p>Returns an iterator over all the nodes in the graph.</p> <p>Returns:</p> Type Description <code>NodeIterator</code> <p>An iterator that yields each node in the graph.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.nodes_colnames","title":"<code>nodes_colnames</code>  <code>property</code>","text":"<p>Returns the column names of the nodes DataFrame.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.number_of_edges","title":"<code>number_of_edges</code>  <code>property</code>","text":"<p>Returns the number of edges in the graph.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of edges in the graph.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.number_of_nodes","title":"<code>number_of_nodes</code>  <code>property</code>","text":"<p>Returns the number of nodes in the graph.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of nodes in the graph.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.out_degree","title":"<code>out_degree</code>  <code>property</code>","text":"<p>Returns the out-degree of each node in the graph as a Python dictionary.</p>"},{"location":"reference/core/#mercury.graph.core.Graph.pagerank","title":"<code>pagerank</code>  <code>property</code>","text":"<p>Returns the PageRank of each node in the graph as a Python dictionary.</p>"},{"location":"reference/core/#mercury.graph.core.Graph._calculate_betweenness_centrality","title":"<code>_calculate_betweenness_centrality()</code>","text":"<p>This internal method handles the logic of a property. It returns the betweenness centrality of each node in the graph as a Python dictionary. NOTE: This method converts the graph to a networkx graph to calculate the betweenness centrality since the algorithm is too computationally expensive to use on large graphs.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _calculate_betweenness_centrality(self):\n    \"\"\"\n    This internal method handles the logic of a property. It returns the betweenness centrality of each node in the graph as a Python\n    dictionary. NOTE: This method converts the graph to a networkx graph to calculate the betweenness centrality since the algorithm\n    is too computationally expensive to use on large graphs.\n    \"\"\"\n    return nx.betweenness_centrality(self.networkx)\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._calculate_closeness_centrality","title":"<code>_calculate_closeness_centrality()</code>","text":"<p>This internal method handles the logic of a property. It returns the closeness centrality of each node in the graph as a Python dictionary.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _calculate_closeness_centrality(self):\n    \"\"\"\n    This internal method handles the logic of a property. It returns the closeness centrality of each node in the graph as\n    a Python dictionary.\n    \"\"\"\n    if self._as_networkx is not None:\n        return nx.closeness_centrality(self._as_networkx)\n\n    nodes = [row['id'] for row in self.graphframe.vertices.select('id').collect()]\n    paths = self.graphframe.shortestPaths(landmarks = nodes)\n    expr  = SparkInterface().pyspark.sql.functions.expr\n    sums  = paths.withColumn('sums', expr('aggregate(map_values(distances), 0, (acc, x) -&gt; acc + x)'))\n\n    cc = sums.withColumn('cc', (self.number_of_nodes - 1)/sums['sums']).select('id', 'cc')\n\n    return {row['id']: row['cc'] for row in cc.collect()}\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._calculate_connected_components","title":"<code>_calculate_connected_components()</code>","text":"<p>This internal method handles the logic of a property. It returns the connected components of each node in the graph as a Python dictionary.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _calculate_connected_components(self):\n    \"\"\"\n    This internal method handles the logic of a property. It returns the connected components of each node in the graph as a Python\n    dictionary.\n    \"\"\"\n    if self._as_networkx is not None:\n        if self._is_directed:\n            G = self._as_networkx.to_undirected()\n        else:\n            G = self._as_networkx\n\n        graphs = (G.subgraph(c) for c in nx.connected_components(G))\n        cc = dict()\n        for i, graph in enumerate(graphs):\n            n = graph.number_of_nodes()\n            for nid in graph.nodes:\n                cc[nid] = {'cc_id' : i, 'cc_size' : n}\n\n        return cc\n\n    graphs = self.graphframe.connectedComponents(algorithm = 'graphx')\n    cc_size = graphs.select('id', 'component').groupBy('component').count()\n    cc_all = graphs.select('id', 'component').join(cc_size, 'component', how = 'left_outer')\n\n    cc = dict()\n    for row in cc_all.collect():\n        cc[row['id']] = {'cc_id' : row['component'], 'cc_size' : row['count']}\n\n    return cc\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._calculate_degree","title":"<code>_calculate_degree()</code>","text":"<p>This internal method handles the logic of a property. It returns the degree of each node in the graph.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _calculate_degree(self):\n    \"\"\" This internal method handles the logic of a property. It returns the degree of each node in the graph.\"\"\"\n\n    if self._as_networkx is not None:\n        return dict(self._as_networkx.degree())\n\n    return self._fill_node_zeros({row['id']: row['degree'] for row in self.graphframe.degrees.collect()})\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._calculate_edges_colnames","title":"<code>_calculate_edges_colnames()</code>","text":"<p>This internal method returns the column names of the edges DataFrame.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _calculate_edges_colnames(self):\n    \"\"\" This internal method returns the column names of the edges DataFrame. \"\"\"\n\n    if self._as_networkx is not None:\n        l = ['src', 'dst']\n        l.extend(list(self._as_networkx.edges[list(self._as_networkx.edges.keys())[0]].keys()))\n        return l\n\n    return self.graphframe.edges.columns\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._calculate_in_degree","title":"<code>_calculate_in_degree()</code>","text":"<p>This internal method handles the logic of a property. It returns the in-degree of each node in the graph.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _calculate_in_degree(self):\n    \"\"\" This internal method handles the logic of a property. It returns the in-degree of each node in the graph.\"\"\"\n\n    if self._as_networkx is not None:\n        return dict(self._as_networkx.in_degree())\n\n    return self._fill_node_zeros({row['id']: row['inDegree'] for row in self.graphframe.inDegrees.collect()})\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._calculate_nodes_colnames","title":"<code>_calculate_nodes_colnames()</code>","text":"<p>This internal method returns the column names of the nodes DataFrame.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _calculate_nodes_colnames(self):\n    \"\"\" This internal method returns the column names of the nodes DataFrame. \"\"\"\n\n    if self._as_networkx is not None:\n        l = ['id']\n        l.extend(list(self._as_networkx.nodes[list(self._as_networkx.nodes.keys())[0]].keys()))\n\n        return l\n\n    return self.graphframe.vertices.columns\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._calculate_out_degree","title":"<code>_calculate_out_degree()</code>","text":"<p>This internal method handles the logic of a property. It returns the out-degree of each node in the graph.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _calculate_out_degree(self):\n    \"\"\" This internal method handles the logic of a property. It returns the out-degree of each node in the graph.\"\"\"\n\n    if self._as_networkx is not None:\n        return dict(self._as_networkx.out_degree())\n\n    return self._fill_node_zeros({row['id']: row['outDegree'] for row in self.graphframe.outDegrees.collect()})\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._calculate_pagerank","title":"<code>_calculate_pagerank()</code>","text":"<p>This internal method handles the logic of a property. It returns the PageRank of each node in the graph as a Python dictionary.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _calculate_pagerank(self):\n    \"\"\"\n    This internal method handles the logic of a property. It returns the PageRank of each node in the graph as a Python dictionary.\n    \"\"\"\n    if self._as_networkx is not None:\n        return nx.pagerank(self._as_networkx)\n\n    pr = self.graphframe.pageRank(resetProbability = 0.15, tol = 0.01).vertices\n\n    return {row['id']: row['pagerank'] for row in pr.collect()}\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._fill_node_zeros","title":"<code>_fill_node_zeros(d)</code>","text":"<p>This internal method fills the nodes that are not in the dictionary with a zero value. This make the output obtained from graphframes consistent with the one from networkx.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _fill_node_zeros(self, d):\n    \"\"\"\n    This internal method fills the nodes that are not in the dictionary with a zero value. This make the output obtained from\n    graphframes consistent with the one from networkx.\n    \"\"\"\n    for node in self.nodes:\n        if node['id'] not in d:\n            d[node['id']] = 0\n\n    return d\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._from_dataframe","title":"<code>_from_dataframe(edges, nodes, keys)</code>","text":"<p>This internal method extends the constructor to accept a pyspark DataFrame as input.</p> <p>It takes the constructor arguments and does not return anything. It sets the internal state of the object.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _from_dataframe(self, edges, nodes, keys):\n    \"\"\" This internal method extends the constructor to accept a pyspark DataFrame as input.\n\n    It takes the constructor arguments and does not return anything. It sets the internal state of the object.\n    \"\"\"\n    if keys is None:\n        src = 'src'\n        dst = 'dst'\n        id  = 'id'\n        weight = 'weight'\n        directed = True\n    else:\n        src = keys.get('src', 'src')\n        dst = keys.get('dst', 'dst')\n        id  = keys.get('id', 'id')\n        weight = keys.get('weight', 'weight')\n        directed = keys.get('directed', True)\n\n    edges = edges.withColumnRenamed(src, 'src').withColumnRenamed(dst, 'dst')\n\n    if weight in edges.columns:\n        edges = edges.withColumnRenamed(weight, 'weight')\n\n    if nodes is not None:\n        nodes = nodes.withColumnRenamed(id, 'id').dropDuplicates(['id'])\n    else:\n        src_nodes = edges.select('src').distinct().withColumnRenamed('src', 'id')\n        dst_nodes = edges.select('dst').distinct().withColumnRenamed('dst', 'id')\n        nodes = src_nodes.union(dst_nodes).distinct()\n\n    g = SparkInterface().graphframes.GraphFrame(nodes, edges)\n\n    if not directed:\n        edges = g.edges\n\n        other_columns = [col for col in edges.columns if col not in ('src', 'dst')]\n        reverse_edges = edges.select(edges['dst'].alias('src'), edges['src'].alias('dst'), *other_columns)\n        all_edges     = edges.union(reverse_edges).distinct()\n\n        g = SparkInterface().graphframes.GraphFrame(nodes, all_edges)\n\n    self._from_graphframes(g, directed)\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._from_graphframes","title":"<code>_from_graphframes(graph, directed=True)</code>","text":"<p>This internal method extends the constructor to accept a graphframes graph as input.</p> <p>It takes the constructor arguments and does not return anything. It sets the internal state of the object.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _from_graphframes(self, graph, directed = True):\n    \"\"\" This internal method extends the constructor to accept a graphframes graph as input.\n\n    It takes the constructor arguments and does not return anything. It sets the internal state of the object.\n    \"\"\"\n    self._as_graphframe = graph\n    self._number_of_nodes = graph.vertices.count()\n    self._number_of_edges = graph.edges.count()\n    self._is_directed = directed\n    self._is_weighted = 'weight' in self.edges_colnames\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._from_networkx","title":"<code>_from_networkx(graph)</code>","text":"<p>This internal method extends the constructor to accept a networkx graph as input.</p> <p>It takes the constructor arguments and does not return anything. It sets the internal state of the object.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _from_networkx(self, graph):\n    \"\"\" This internal method extends the constructor to accept a networkx graph as input.\n\n    It takes the constructor arguments and does not return anything. It sets the internal state of the object.\n    \"\"\"\n    self._as_networkx = graph\n    self._number_of_nodes = len(graph.nodes)\n    self._number_of_edges = len(graph.edges)\n    self._is_directed = nx.is_directed(graph)\n    self._is_weighted = 'weight' in self.edges_colnames\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._from_pandas","title":"<code>_from_pandas(edges, nodes, keys)</code>","text":"<p>This internal method extends the constructor to accept a pandas DataFrame as input.</p> <p>It takes the constructor arguments and does not return anything. It sets the internal state of the object.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _from_pandas(self, edges, nodes, keys):\n    \"\"\" This internal method extends the constructor to accept a pandas DataFrame as input.\n\n    It takes the constructor arguments and does not return anything. It sets the internal state of the object.\n    \"\"\"\n    if keys is None:\n        src = 'src'\n        dst = 'dst'\n        id  = 'id'\n        weight = 'weight'\n        directed = True\n    else:\n        src = keys.get('src', 'src')\n        dst = keys.get('dst', 'dst')\n        id  = keys.get('id', 'id')\n        weight = keys.get('weight', 'weight')\n        directed = keys.get('directed', True)\n\n    if directed:\n        g = nx.DiGraph()\n    else:\n        g = nx.Graph()\n\n    if weight in edges.columns:\n        edges = edges.rename(columns = {weight: 'weight'})\n\n    for _, row in edges.iterrows():\n        attr = row.drop([src, dst]).to_dict()\n        g.add_edge(row[src], row[dst], **attr)\n\n    if nodes is not None:\n        for _, row in nodes.iterrows():\n            attr = row.drop([id]).to_dict()\n            g.add_node(row[id], **attr)\n\n    self._from_networkx(g)\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._to_dgl","title":"<code>_to_dgl()</code>","text":"<p>This internal method handles the logic of a property. It returns the dgl graph that already exists or converts it from the networkx graph if not.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _to_dgl(self):\n    \"\"\" This internal method handles the logic of a property. It returns the dgl graph that already exists\n    or converts it from the networkx graph if not.\"\"\"\n\n    if dgl_installed:\n        dgl = SparkInterface().dgl\n\n        edge_attrs = [c for c in self.edges_colnames if c not in ['src', 'dst']]\n        if len(edge_attrs) == 0:\n            edge_attrs = None\n\n        node_attrs = [c for c in self.nodes_colnames if c not in ['id']]\n        if len(node_attrs) == 0:\n            node_attrs = None\n\n        self._as_dgl = dgl.from_networkx(self.networkx, edge_attrs = edge_attrs, node_attrs = node_attrs)\n\n    return self._as_dgl\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._to_graphframe","title":"<code>_to_graphframe()</code>","text":"<p>This internal method handles the logic of a property. It returns the graphframes graph that already exists or converts it from the networkx graph if not.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _to_graphframe(self):\n    \"\"\" This internal method handles the logic of a property. It returns the graphframes graph that already exists\n    or converts it from the networkx graph if not.\"\"\"\n\n    nodes = self.nodes_as_dataframe()\n    edges = self.edges_as_dataframe()\n\n    return SparkInterface().graphframes.GraphFrame(nodes, edges)\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph._to_networkx","title":"<code>_to_networkx()</code>","text":"<p>This internal method handles the logic of a property. It returns the networkx graph that already exists or converts it from the graphframes graph if not.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def _to_networkx(self):\n    \"\"\" This internal method handles the logic of a property. It returns the networkx graph that already exists\n    or converts it from the graphframes graph if not.\"\"\"\n\n    if self._is_directed:\n        g = nx.DiGraph()\n    else:\n        g = nx.Graph()\n\n    for _, row in self.edges_as_pandas().iterrows():\n        attr = row.drop(['src', 'dst']).to_dict()\n        g.add_edge(row['src'], row['dst'], **attr)\n\n    for _, row in self.nodes_as_pandas().iterrows():\n        attr = row.drop(['id']).to_dict()\n        g.add_node(row['id'], **attr)\n\n    return g\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph.edges_as_dataframe","title":"<code>edges_as_dataframe()</code>","text":"<p>Returns the edges as a pyspark DataFrame.</p> <p>If the graph is represented as a graphframes graph, the edges are extracted from it. Otherwise, the edges are converted from the pandas DataFrame representation. The columns used as the source and destination nodes are always named 'src' and 'dst', respectively, regardless of the original column names passed to the constructor.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def edges_as_dataframe(self):\n    \"\"\"\n    Returns the edges as a pyspark DataFrame.\n\n    If the graph is represented as a graphframes graph, the edges are extracted from it. Otherwise, the edges are converted from the\n    pandas DataFrame representation. The columns used as the source and destination nodes are always named 'src' and 'dst',\n    respectively, regardless of the original column names passed to the constructor.\n    \"\"\"\n    if self._as_graphframe is not None:\n        return self._as_graphframe.edges\n\n    return SparkInterface().spark.createDataFrame(self.edges_as_pandas())\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph.edges_as_pandas","title":"<code>edges_as_pandas()</code>","text":"<p>Returns the edges as a pandas DataFrame.</p> <p>If the graph is represented as a networkx graph, the edges are extracted from it. Otherwise, the graphframes graph will be used. This dataset may differ from possible pandas DataFrame passed to the constructor in the column names and order. The columns used as the source and destination nodes are always named 'src' and 'dst', respectively.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def edges_as_pandas(self):\n    \"\"\"\n    Returns the edges as a pandas DataFrame.\n\n    If the graph is represented as a networkx graph, the edges are extracted from it. Otherwise, the graphframes graph will be used.\n    This dataset may differ from possible pandas DataFrame passed to the constructor in the column names and order. The columns used\n    as the source and destination nodes are always named 'src' and 'dst', respectively.\n    \"\"\"\n    if self._as_networkx is not None:\n        edges_data = self._as_networkx.edges(data = True)\n        edges_df   = pd.DataFrame([(src, dst, attr) for src, dst, attr in edges_data], columns = ['src', 'dst', 'attributes'])\n\n        attrs_df   = pd.json_normalize(edges_df['attributes'])\n\n        return pd.concat([edges_df.drop('attributes', axis = 1), attrs_df], axis = 1)\n\n    return self.graphframe.edges.toPandas()\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph.nodes_as_dataframe","title":"<code>nodes_as_dataframe()</code>","text":"<p>Returns the nodes as a pyspark DataFrame.</p> <p>If the graph is represented as a graphframes graph, the nodes are extracted from it. Otherwise, the nodes are converted from the pandas DataFrame representation. The column used as the node id is always named 'id', regardless of the original column name passed to the constructor.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def nodes_as_dataframe(self):\n    \"\"\"\n    Returns the nodes as a pyspark DataFrame.\n\n    If the graph is represented as a graphframes graph, the nodes are extracted from it. Otherwise, the nodes are converted from the\n    pandas DataFrame representation. The column used as the node id is always named 'id', regardless of the original column name passed\n    to the constructor.\n    \"\"\"\n    if self._as_graphframe is not None:\n        return self._as_graphframe.vertices\n\n    return SparkInterface().spark.createDataFrame(self.nodes_as_pandas())\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.Graph.nodes_as_pandas","title":"<code>nodes_as_pandas()</code>","text":"<p>Returns the nodes as a pandas DataFrame.</p> <p>If the graph is represented as a networkx graph, the nodes are extracted from it. Otherwise, the graphframes graph will be used. This dataset may differ from possible pandas DataFrame passed to the constructor in the column names and order. The column used as the node id is always named 'id'.</p> Source code in <code>mercury/graph/core/graph.py</code> <pre><code>def nodes_as_pandas(self):\n    \"\"\"\n    Returns the nodes as a pandas DataFrame.\n\n    If the graph is represented as a networkx graph, the nodes are extracted from it. Otherwise, the graphframes graph will be used.\n    This dataset may differ from possible pandas DataFrame passed to the constructor in the column names and order. The column used\n    as the node id is always named 'id'.\n    \"\"\"\n    if self._as_networkx is not None:\n        nodes_data = self._as_networkx.nodes(data = True)\n        nodes_df   = pd.DataFrame([(node, attr) for node, attr in nodes_data], columns = ['id', 'attributes'])\n\n        attrs_df = pd.json_normalize(nodes_df['attributes'])\n\n        return pd.concat([nodes_df.drop('attributes', axis = 1), attrs_df], axis = 1)\n\n    return self.graphframe.vertices.toPandas()\n</code></pre>"},{"location":"reference/core/#mercury.graph.core.SparkInterface","title":"<code>mercury.graph.core.SparkInterface(config=None, session=None)</code>","text":"<p>A class that provides an interface for interacting with Apache Spark, graphframes and dgl.</p> <p>Attributes:</p> Name Type Description <code>_spark_session</code> <code>SparkSession</code> <p>The shared Spark session.</p> <code>_graphframes</code> <code>module</code> <p>The shared graphframes namespace.</p> <p>Methods:</p> Name Description <code>_create_spark_session</code> <p>Creates a Spark session.</p> <code>spark</code> <p>Property that returns the shared Spark session.</p> <code>pyspark</code> <p>Property that returns the pyspark namespace.</p> <code>graphframes</code> <p>Property that returns the shared graphframes namespace.</p> <code>dgl</code> <p>Property that returns the shared dgl namespace.</p> <code>read_csv</code> <p>Reads a CSV file into a DataFrame.</p> <code>read_parquet</code> <p>Reads a Parquet file into a DataFrame.</p> <code>read_json</code> <p>Reads a JSON file into a DataFrame.</p> <code>read_text</code> <p>Reads a text file into a DataFrame.</p> <code>read</code> <p>Reads a file into a DataFrame.</p> <code>sql</code> <p>Executes a SQL query.</p> <code>udf</code> <p>Registers a user-defined function (UDF).</p> <code>stop</code> <p>Stops the Spark session.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary of Spark configuration options. If not provided, the configuration in the global variable <code>default_spark_config</code> will be used.</p> <code>None</code> Source code in <code>mercury/graph/core/spark_interface.py</code> <pre><code>def __init__(self, config=None, session=None):\n    if SparkInterface._spark_session is None:\n        if session is not None:\n            SparkInterface._spark_session = session\n        else:\n            SparkInterface._spark_session = self._create_spark_session(config)\n            # Set checkpoint directory\n            SparkInterface._spark_session.sparkContext.setCheckpointDir(\".checkpoint\")\n\n    if SparkInterface._graphframes is None and graphframes_installed:\n        SparkInterface._graphframes = gf\n\n    if SparkInterface._dgl is None and dgl_installed:\n        SparkInterface._dgl = dgl\n</code></pre>"},{"location":"reference/embeddings/","title":"mercury.graph.embeddings","text":""},{"location":"reference/embeddings/#mercury.graph.embeddings.Embeddings","title":"<code>mercury.graph.embeddings.Embeddings(dimension, num_elements=0, mean=0, sd=1, learn_step=3, bidirectional=False)</code>","text":"<p>               Bases: <code>BaseClass</code></p> <p>This class holds a matrix object that is interpreted as the embeddings for any list of objects, not only the nodes of a graph. You can see this class as the internal object holding the embedding for other classes such as class GraphEmbedding.</p> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>int</code> <p>The number of columns in the embedding. See note below.</p> required <code>num_elements</code> <code>int</code> <p>The number of rows in the embedding. You can leave this empty on creation and then use initialize_as() to automatically match the nodes in a graph.</p> <code>0</code> <code>mean</code> <code>float</code> <p>The (expected) mean of the initial values.</p> <code>0</code> <code>sd</code> <code>float</code> <p>The (expected) standard deviation of the initial values.</p> <code>1</code> <code>learn_step</code> <code>float</code> <p>The size of the learning step elements get approached or moved away. Units are hexadecimal degrees in along an ellipse.</p> <code>3</code> <code>bidirectional</code> <code>bool</code> <p>Should the changes apply only to the elements of first column (False) or to both.</p> <code>False</code> Note <p>On dimension: Embeddings cannot be zero (that is against the whole concept). Smaller dimension embeddings can only hold few elements without introducing spurious correlations by some form of 'birthday attack' phenomenon as elements increase. Later it is very hard to get rid of that spurious 'knowledge'. </p> <p>Solution: With may elements, you have to go to high enough dimension even if the structure is simple.  Pretending to fit many embeddings in low dimension without them being correlated is like pretending to plot a trillion random  points in a square centimeter while keeping them 1 mm apart from each other: It's simply impossible!</p> Source code in <code>mercury/graph/embeddings/embeddings.py</code> <pre><code>def __init__(\n    self, dimension, num_elements=0, mean=0, sd=1, learn_step=3, bidirectional=False\n):\n    self.dimension = dimension\n    self.num_elements = num_elements\n    self.mean = mean\n    self.sd = sd\n    self.learn_step = learn_step\n    self.bidirectional = bidirectional\n\n    if self.num_elements &gt; 0:\n        self.embeddings_matrix_ = np.random.normal(\n            self.mean, self.sd, (self.num_elements, self.dimension)\n        )\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.Embeddings.as_numpy","title":"<code>as_numpy()</code>","text":"<p>Return the embedding as a numpy matrix where each row is an embedding.</p> Source code in <code>mercury/graph/embeddings/embeddings.py</code> <pre><code>def as_numpy(self):\n    \"\"\"\n    Return the embedding as a numpy matrix where each row is an embedding.\n    \"\"\"\n    if not hasattr(self, \"embeddings_matrix_\"):\n        return\n\n    return self.embeddings_matrix_\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.Embeddings.fit","title":"<code>fit(converge=None, diverge=None)</code>","text":"<p>Apply a learning step to the embedding.</p> <p>Parameters:</p> Name Type Description Default <code>converge</code> <code>numpy matrix of two columns</code> <p>A matrix of indices to elements meaning (first column) should be approached to (second column).</p> <code>None</code> <code>diverge</code> <code>numpy matrix of two columns</code> <p>A matrix of indices to elements meaning (first column) should be moved away from (second column).</p> <code>None</code> <p>Returns:</p> Type Description <code>self</code> <p>Fitted self (or raises an error)</p> Note <p>Embeddings start being randomly distributed and hold no structure other than spurious correlations. Each time you apply a learning step by calling this method, you are tweaking the embedding to approach some rows and/or move others away. You can use both converge and diverge or just one of them and call this as many times you want with varying learning step. A proxy of how much an embedding can learn can be estimated by measuring how row correlations are converging towards some asymptotic values.</p> Source code in <code>mercury/graph/embeddings/embeddings.py</code> <pre><code>def fit(self, converge=None, diverge=None):\n    \"\"\"\n    Apply a learning step to the embedding.\n\n    Args:\n        converge (numpy matrix of two columns): A matrix of indices to elements meaning (first column) should be approached to\n            (second column).\n        diverge (numpy matrix of two columns): A matrix of indices to elements meaning (first column) should be moved away from\n            (second column).\n\n    Returns:\n        (self): Fitted self (or raises an error)\n\n    Note:\n        Embeddings start being randomly distributed and hold no structure other than spurious correlations. Each time you apply a\n        learning step by calling this method, you are tweaking the embedding to approach some rows and/or move others away. You can use\n        both converge and diverge or just one of them and call this as many times you want with varying learning step. A proxy of how\n        much an embedding can learn can be estimated by measuring how row correlations are converging towards some asymptotic values.\n    \"\"\"\n\n    w = self.learn_step * np.pi / 180\n\n    cos_w = np.cos(w)\n    sin_w = np.sin(w)\n\n    if converge is not None:\n        self.embeddings_matrix_ = _elliptic_rotate(\n            self.embeddings_matrix_, converge[:, 0], converge[:, 1], cos_w, sin_w\n        )\n\n        if self.bidirectional:\n            self.embeddings_matrix_ = _elliptic_rotate(\n                self.embeddings_matrix_,\n                converge[:, 1],\n                converge[:, 0],\n                cos_w,\n                sin_w,\n            )\n\n    if diverge is not None:\n        self.embeddings_matrix_ = _elliptic_rotate(\n            self.embeddings_matrix_, diverge[:, 0], diverge[:, 1], cos_w, -sin_w\n        )\n\n        if self.bidirectional:\n            self.embeddings_matrix_ = _elliptic_rotate(\n                self.embeddings_matrix_, diverge[:, 1], diverge[:, 0], cos_w, -sin_w\n            )\n\n    return self\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.Embeddings.get_most_similar_embeddings","title":"<code>get_most_similar_embeddings(index, k=5, metric='cosine')</code>","text":"<p>Given an index of a vector in the embedding matrix, returns the k most similar embeddings in the matrix</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>index of the vector in the matrix that we want to compute the similar embeddings</p> required <code>k</code> <code>int</code> <p>Number of most similar embeddings to return</p> <code>5</code> <code>metric</code> <code>str</code> <p>metric to use as a similarity.</p> <code>'cosine'</code> <p>Returns:</p> Type Description <code>list</code> <p>list of k most similar nodes as indices and list of similarities of the most similar nodes</p> Source code in <code>mercury/graph/embeddings/embeddings.py</code> <pre><code>def get_most_similar_embeddings(self, index, k=5, metric=\"cosine\"):\n    \"\"\"\n    Given an index of a vector in the embedding matrix, returns the k most similar embeddings in the matrix\n\n    Args:\n        index (int): index of the vector in the matrix that we want to compute the similar embeddings\n        k (int): Number of most similar embeddings to return\n        metric (str): metric to use as a similarity.\n\n    Returns:\n        (list): list of k most similar nodes as indices and list of similarities of the most similar nodes\n    \"\"\"\n    if metric == \"cosine\":\n        similarities = (\n            1\n            - cdist(\n                np.expand_dims(self.as_numpy()[index], axis=0),\n                self.as_numpy(),\n                \"cosine\",\n            )[0]\n        )\n\n    elif metric == \"euclidean\":\n        similarities = 1 / (\n            1\n            + cdist(\n                np.expand_dims(self.as_numpy()[index], axis=0),\n                self.as_numpy(),\n                \"euclidean\",\n            )[0]\n        )\n\n    else:\n        raise ValueError(\"Unknown Distance Metric: %s\" % metric)\n\n    ordered_indices = np.argsort(similarities)[::-1][1 : (k + 1)]\n    ordered_similarities = similarities[ordered_indices]\n\n    return ordered_indices, ordered_similarities\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.GraphEmbedding","title":"<code>mercury.graph.embeddings.GraphEmbedding(dimension=None, n_jumps=None, max_per_epoch=None, learn_step=3, bidirectional=False, load_file=None)</code>","text":"<p>               Bases: <code>BaseClass</code></p> <p>Create an embedding mapping the nodes of a graph.</p> <p>Includes contributions by David Muelas Recuenco.</p> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>int</code> <p>The number of columns in the embedding. See note the notes in <code>Embeddings</code> for details. (This parameter will be ignored when <code>load_file</code> is used.)</p> <code>None</code> <code>n_jumps</code> <code>int</code> <p>Number of random jumps from node to node.</p> <code>None</code> <code>max_per_epoch</code> <code>int</code> <p>Maximum number Number of consecutive random jumps without randomly jumping outside the edges. Note that normal random jumps are not going to explore outside a connected component.</p> <code>None</code> <code>learn_step</code> <code>float</code> <p>The size of the learning step elements get approached or moved away. Units are hexadecimal degrees in along an ellipse.</p> <code>3</code> <code>bidirectional</code> <code>bool</code> <p>Should the changes apply only to the elements of first column (False) or to both.</p> <code>False</code> <code>load_file</code> <code>str</code> <p>(optional) The full path to a binary file containing a serialized GraphEmbedding object. This file must be created using GraphEmbedding.save().</p> <code>None</code> <p>GraphEmbedding class constructor</p> Source code in <code>mercury/graph/embeddings/graphembeddings.py</code> <pre><code>def __init__(\n    self,\n    dimension=None,\n    n_jumps=None,\n    max_per_epoch=None,\n    learn_step=3,\n    bidirectional=False,\n    load_file=None,\n):\n    \"\"\"GraphEmbedding class constructor\"\"\"\n    if load_file is None and (dimension is None or n_jumps is None):\n        raise ValueError(\n            \"Parameters dimension and n_jumps are required when load_file is None\"\n        )\n\n    self.dimension = dimension\n    self.n_jumps = n_jumps\n    self.max_per_epoch = max_per_epoch\n    self.learn_step = learn_step\n    self.bidirectional = bidirectional\n    self.load_file = load_file\n\n    if self.load_file is not None:\n        self._load(self.load_file)\n        return\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.GraphEmbedding.__getitem__","title":"<code>__getitem__(arg)</code>","text":"<p>Method to access rows in the embedding by ID.</p> <p>Parameters:</p> Name Type Description Default <code>arg</code> <code>same as node ids in the graph</code> <p>A node ID in the graph</p> required <p>Returns:</p> Type Description <code>matrix</code> <p>A numpy matrix of one row</p> Source code in <code>mercury/graph/embeddings/graphembeddings.py</code> <pre><code>def __getitem__(self, arg):\n    \"\"\"\n    Method to access rows in the embedding by ID.\n\n    Args:\n        arg (same as node ids in the graph): A node ID in the graph\n\n    Returns:\n        (numpy.matrix): A numpy matrix of one row\n\n    \"\"\"\n    return self.embeddings_.embeddings_matrix_[self.node_ids.index(arg)]\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.GraphEmbedding._load","title":"<code>_load(file_name)</code>","text":"<p>This method is internal and should not be called directly. Use the constructor's <code>load_file</code> argument instead. E.g., <code>ge = GraphEmbedding(load_file = 'some/stored/embedding')</code></p> Source code in <code>mercury/graph/embeddings/graphembeddings.py</code> <pre><code>def _load(self, file_name):\n    \"\"\"\n    This method is internal and should not be called directly. Use the constructor's `load_file` argument instead.\n    E.g., `ge = GraphEmbedding(load_file = 'some/stored/embedding')`\n    \"\"\"\n    with bz2.BZ2File(file_name, \"r\") as f:\n        head = pickle.load(f)\n\n        if head != GraphEmbedding.FILE_HEAD:\n            raise ValueError(\"Unsupported file format!\")\n\n        has_emb = pickle.load(f)\n        dimension = pickle.load(f)\n\n        self.node_ids = pickle.load(f)\n\n        self.r_ini = np.load(f)\n        self.r_len = np.load(f)\n        self.r_sum = np.load(f)\n        self.r_col = np.load(f)\n        self.r_wgt = np.load(f)\n\n        self.TotW = pickle.load(f)\n\n        self.embeddings_ = Embeddings(dimension, len(self.node_ids))\n\n        if has_emb:\n            self.embeddings_.embeddings_matrix_ = np.load(f)\n\n        end = pickle.load(f)\n\n        if end != GraphEmbedding.FILE_END:\n            raise ValueError(\"Unsupported file format!\")\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.GraphEmbedding.embedding","title":"<code>embedding()</code>","text":"<p>Return the internal Embeddings object.</p> <p>Returns:</p> Type Description <code>Embeddings</code> <p>The embedding which is a dense matrix of <code>float</code> that can be used with <code>numpy</code> functions.</p> Source code in <code>mercury/graph/embeddings/graphembeddings.py</code> <pre><code>def embedding(self):\n    \"\"\"\n    Return the internal Embeddings object.\n\n    Returns:\n        (mercury.graph.embeddings.Embeddings): The embedding which is a dense matrix of `float` that can be used with `numpy` functions.\n    \"\"\"\n    if not hasattr(self, \"embeddings_\"):\n        return\n\n    return self.embeddings_\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.GraphEmbedding.fit","title":"<code>fit(g)</code>","text":"<p>Train the embedding by doing random walks.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>mercury.graph Graph asset</code> <p>A <code>mercury.graph</code> Graph object. The embedding will be created so that each row in the embedding maps a node ID in g.</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted self (or raises an error)</p> <p>This does a number of random walks starting from a random node and selecting the edges with a probability that is proportional to the weight of the edge. If the destination node also has outgoing edges, the next step will start from it, otherwise, a new random node will be selected. The edges visited (concordant pairs) will get some reinforcement in the embedding while a randomly selected non-existent edges will get divergence instead (discordant pairs).</p> <p>Internally, this stores the node IDS of the node visited and calls Embeddings.fit() to transfer the structure to the embedding. Of course, it can be called many times on the same GraphEmbedding.</p> Source code in <code>mercury/graph/embeddings/graphembeddings.py</code> <pre><code>def fit(self, g: Graph):\n    \"\"\"\n    Train the embedding by doing random walks.\n\n    Args:\n        g (mercury.graph Graph asset): A `mercury.graph` Graph object. The embedding will be created so that each row in the embedding maps\n            a node ID in g.\n\n    Returns:\n        (self): Fitted self (or raises an error)\n\n    This does a number of random walks starting from a random node and selecting the edges with a probability that is proportional to\n    the weight of the edge. If the destination node also has outgoing edges, the next step will start from it, otherwise, a new random\n    node will be selected. The edges visited (concordant pairs) will get some reinforcement in the embedding while a randomly selected\n    non-existent edges will get divergence instead (discordant pairs).\n\n    Internally, this stores the node IDS of the node visited and calls Embeddings.fit() to transfer the structure to the embedding.\n    Of course, it can be called many times on the same GraphEmbedding.\n\n    \"\"\"\n\n    self.node_ids = list(g.networkx.nodes)\n\n    j_matrix = nx.adjacency_matrix(g.networkx)\n\n    N = j_matrix.shape[1]\n    M = j_matrix.nnz\n\n    self.r_ini = np.zeros(N, dtype=int)\n    self.r_len = np.zeros(N, dtype=int)\n    self.r_sum = np.zeros(N, dtype=float)\n    self.r_col = np.zeros(M, dtype=int)\n    self.r_wgt = np.zeros(M, dtype=float)\n\n    i = 0\n    for r in range(N):\n        self.r_ini[r] = i\n\n        i_col = j_matrix[[r], :].nonzero()[1]\n        L = len(i_col)\n\n        self.r_len[r] = L\n\n        for k in range(L):\n            c = i_col[k]\n            w = j_matrix[r, c]\n\n            self.r_sum[r] += w\n            self.r_col[i] = c\n            self.r_wgt[i] = w\n\n            i += 1\n\n    self.TotW = sum(self.r_sum)\n\n    converge, diverge = _random_walks(\n        self.r_ini,\n        self.r_len,\n        self.r_sum,\n        self.r_col,\n        self.r_wgt,\n        self.TotW,\n        self.n_jumps,\n        self.max_per_epoch if self.max_per_epoch is not None else self.n_jumps,\n    )\n\n    self.embeddings_ = Embeddings(\n        dimension=self.dimension,\n        num_elements=len(self.node_ids),\n        learn_step=self.learn_step,\n        bidirectional=self.bidirectional,\n    )\n    self.embeddings_.fit(converge, diverge)\n\n    return self\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.GraphEmbedding.get_most_similar_nodes","title":"<code>get_most_similar_nodes(node_id, k=5, metric='cosine', return_as_indices=False)</code>","text":"<p>Returns the k most similar nodes and the similarities</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>object</code> <p>Id of the node that we want to search the similar nodes.</p> required <code>k</code> <code>int</code> <p>Number of most similar nodes to return</p> <code>5</code> <code>metric</code> <code>str</code> <p>metric to use as a similarity.</p> <code>'cosine'</code> <code>return_as_indices</code> <code>bool</code> <p>if return the nodes as indices (False), or as node ids (True)</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>list of k most similar nodes and list of similarities of the most similar nodes</p> <code>DataFrame</code> <p>A list of k most similar nodes as a <code>pd.DataFrame[word: string, similarity: double]</code></p> Source code in <code>mercury/graph/embeddings/graphembeddings.py</code> <pre><code>def get_most_similar_nodes(\n    self, node_id, k=5, metric=\"cosine\", return_as_indices=False\n):\n    \"\"\"\n    Returns the k most similar nodes and the similarities\n\n    Args:\n        node_id (object): Id of the node that we want to search the similar nodes.\n        k (int): Number of most similar nodes to return\n        metric (str): metric to use as a similarity.\n        return_as_indices (bool): if return the nodes as indices (False), or as node ids (True)\n\n    Returns:\n        (list): list of k most similar nodes and list of similarities of the most similar nodes\n        (DataFrame): A list of k most similar nodes as a `pd.DataFrame[word: string, similarity: double]`\n    \"\"\"\n    node_index = self.node_ids.index(node_id)\n\n    ordered_indices, ordered_similarities = (\n        self.embeddings_.get_most_similar_embeddings(node_index, k, metric)\n    )\n\n    if not return_as_indices:\n        nodes = list(np.array(self.node_ids)[ordered_indices])\n    else:\n        nodes = list(ordered_indices)\n\n    return pd.DataFrame({\"word\": nodes, \"similarity\": ordered_similarities})\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.GraphEmbedding.save","title":"<code>save(file_name, save_embedding=False)</code>","text":"<p>Saves a GraphEmbedding to a compressed binary file with or without the embedding itself. It saves the graph's node names and the adjacency matrix as a sparse matrix.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to which the GraphEmbedding will be saved.</p> required <code>save_embedding</code> <code>bool</code> <p>Since the embedding can be big and, if not trained, it is just a matrix of uniform random numbers it is possible avoiding saving it. In case it is not saved, loading the file will create a new random embedding. This parameter controls if the embedding is saved or not (the default value).</p> <code>False</code> Source code in <code>mercury/graph/embeddings/graphembeddings.py</code> <pre><code>def save(self, file_name, save_embedding=False):\n    \"\"\"\n    Saves a GraphEmbedding to a compressed binary file with or without the embedding itself. It saves the graph's node names\n    and the adjacency matrix as a sparse matrix.\n\n    Args:\n        file_name (str): The name of the file to which the GraphEmbedding will be saved.\n        save_embedding (bool): Since the embedding can be big and, if not trained, it is just a matrix of uniform random numbers it is\n            possible avoiding saving it. In case it is not saved, loading the file will create a new random embedding. This parameter\n            controls if the embedding is saved or not (the default value).\n    \"\"\"\n    with bz2.BZ2File(file_name, \"w\") as f:\n        pickle.dump(GraphEmbedding.FILE_HEAD, f)\n        pickle.dump(save_embedding, f)\n        pickle.dump(self.embeddings_.dimension, f)\n\n        pickle.dump(self.node_ids, f)\n\n        np.save(f, self.r_ini)\n        np.save(f, self.r_len)\n        np.save(f, self.r_sum)\n        np.save(f, self.r_col)\n        np.save(f, self.r_wgt)\n\n        pickle.dump(self.TotW, f)\n\n        if save_embedding:\n            np.save(f, self.embeddings_.embeddings_matrix_)\n\n        pickle.dump(GraphEmbedding.FILE_END, f)\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.SparkNode2Vec","title":"<code>mercury.graph.embeddings.SparkNode2Vec(dimension=None, sampling_ratio=1.0, num_epochs=10, num_paths_per_node=1, batch_size=1000000, w2v_max_iter=1, w2v_num_partitions=1, w2v_step_size=0.025, w2v_min_count=5, path_cache=None, use_cached_rw=False, n_partitions_cache=10, load_file=None)</code>","text":"<p>               Bases: <code>BaseClass</code></p> <p>Create or reload a SparkNode2Vec embedding mapping the nodes of a graph.</p> <p>Parameters:</p> Name Type Description Default <code>dimension</code> <code>int</code> <p>The number of columns in the embedding. See note the notes in <code>Embeddings</code> for details. (This parameter will be ignored when <code>load_file</code> is used.)</p> <code>None</code> <code>sampling_ratio</code> <code>float</code> <p>The proportion from the total number of nodes to be used in parallel at each step (whenever possible).</p> <code>1.0</code> <code>num_epochs</code> <code>int</code> <p>Number of epochs. This is the total number of steps the iteration goes through. At each step, sampling_ratio times the total number of nodes paths will be computed in parallel.</p> <code>10</code> <code>num_paths_per_node</code> <code>int</code> <p>The amount of random walks to source from each node.</p> <code>1</code> <code>batch_size</code> <code>int</code> <p>This forces caching the random walks computed so far and breaks planning each time this number of epochs is reached. The default value is a high number to avoid this entering at all. In really large jobs, you may want to set this parameter to avoid possible overflows even if it can add some extra time to the process. Note that with a high number of epochs and nodes resource requirements for the active part of your random walks can be high. This allows to \"cache a continue\" so to say.</p> <code>1000000</code> <code>w2v_max_iter</code> <code>int</code> <p>This is the Spark Word2Vec parameter maxIter, the default value is the original default value.</p> <code>1</code> <code>w2v_num_partitions</code> <code>int</code> <p>This is the Spark Word2Vec parameter numPartitions, the default value is the original default value.</p> <code>1</code> <code>w2v_step_size</code> <code>float</code> <p>This is the Spark Word2Vec parameter stepSize, the default value is the original default value.</p> <code>0.025</code> <code>w2v_min_count</code> <code>int</code> <p>This is the Spark Word2Vec parameter minCount, the default value is the original default value (5). Is the minimum number of times that a node has to appear to generate an embedding.</p> <code>5</code> <code>path_cache</code> <code>str</code> <p>Folder where random walks will be stored, the default value is None which entails that random walks will not be stored.</p> <code>None</code> <code>use_cached_rw</code> <code>bool</code> <p>Flag that indicates if random walks should be read from disk (hence, they will not be computed again). Setting this parameter to True requires a valid path_cache.</p> <code>False</code> <code>n_partitions_cache</code> <code>int</code> <p>Number of partitions that will be used when storing the random walks, to optimize read access. The default value is 10.</p> <code>10</code> <code>load_file</code> <code>str</code> <p>(optional) The full path to a parquet file containing a serialized SparkNode2Vec object. This file must be created using SparkNode2Vec.save().</p> <code>None</code> Source code in <code>mercury/graph/embeddings/spark_node2vec.py</code> <pre><code>def __init__(\n    self,\n    dimension=None,\n    sampling_ratio=1.0,\n    num_epochs=10,\n    num_paths_per_node=1,\n    batch_size=1000000,\n    w2v_max_iter=1,\n    w2v_num_partitions=1,\n    w2v_step_size=0.025,\n    w2v_min_count=5,\n    path_cache=None,\n    use_cached_rw=False,\n    n_partitions_cache=10,\n    load_file=None,\n):\n    \"\"\"\n    Create or reload a SparkNode2Vec embedding mapping the nodes of a graph.\n\n    Args:\n        dimension (int): The number of columns in the embedding. See note the notes in `Embeddings` for details. (This parameter will be\n            ignored when `load_file` is used.)\n        sampling_ratio (float): The proportion from the total number of nodes to be used in parallel at each step (whenever possible).\n        num_epochs (int): Number of epochs. This is the total number of steps the iteration goes through. At each step, sampling_ratio\n            times the total number of nodes paths will be computed in parallel.\n        num_paths_per_node (int): The amount of random walks to source from each node.\n        batch_size (int): This forces caching the random walks computed so far and breaks planning each time this number of epochs\n            is reached. The default value is a high number to avoid this entering at all. In really large jobs, you may want to\n            set this parameter to avoid possible overflows even if it can add some extra time to the process. Note that with a high\n            number of epochs and nodes resource requirements for the active part of your random walks can be high. This allows to\n            \"cache a continue\" so to say.\n        w2v_max_iter (int): This is the Spark Word2Vec parameter maxIter, the default value is the original default value.\n        w2v_num_partitions (int): This is the Spark Word2Vec parameter numPartitions, the default value is the original default value.\n        w2v_step_size (float): This is the Spark Word2Vec parameter stepSize, the default value is the original default value.\n        w2v_min_count (int): This is the Spark Word2Vec parameter minCount, the default value is the original default value (5). Is the\n            minimum number of times that a node has to appear to generate an embedding.\n        path_cache (str): Folder where random walks will be stored, the default value is None which entails that random walks will not\n            be stored.\n        use_cached_rw (bool): Flag that indicates if random walks should be read from disk (hence, they will not be computed again).\n            Setting this parameter to True requires a valid path_cache.\n        n_partitions_cache (int): Number of partitions that will be used when storing the random walks, to optimize read access.\n            The default value is 10.\n        load_file (str): (optional) The full path to a parquet file containing a serialized SparkNode2Vec object. This file must be created\n            using SparkNode2Vec.save().\n    \"\"\"\n    self.dimension = dimension\n    self.sampling_ratio = sampling_ratio\n    self.num_epochs = num_epochs\n    self.num_paths_per_node = num_paths_per_node\n    self.batch_size = batch_size\n    self.w2v_max_iter = w2v_max_iter\n    self.w2v_num_partitions = w2v_num_partitions\n    self.w2v_step_size = w2v_step_size\n    self.w2v_min_count = w2v_min_count\n    self.path_cache = path_cache\n    self.use_cached_rw = use_cached_rw\n    self.n_partitions_cache = n_partitions_cache\n    self.load_file = load_file\n\n    if self.load_file is not None:\n        self._load(self.load_file)\n        return\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.SparkNode2Vec._load","title":"<code>_load(file_name)</code>","text":"<p>This method is internal and should not be called directly. Use the constructor's <code>load_file</code> argument instead. E.g., <code>snv = SparkNode2Vec(load_file = 'some/stored/embedding')</code></p> Source code in <code>mercury/graph/embeddings/spark_node2vec.py</code> <pre><code>def _load(self, file_name):\n    \"\"\"\n    This method is internal and should not be called directly. Use the constructor's `load_file` argument instead.\n    E.g., `snv = SparkNode2Vec(load_file = 'some/stored/embedding')`\n    \"\"\"\n\n    self.node2vec_ = Word2VecModel.load(file_name)\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.SparkNode2Vec.embedding","title":"<code>embedding()</code>","text":"<p>Return all embeddings.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>All embeddings as a <code>DataFrame[word: string, vector: vector]</code>.</p> Source code in <code>mercury/graph/embeddings/spark_node2vec.py</code> <pre><code>def embedding(self):\n    \"\"\"\n    Return all embeddings.\n\n    Returns:\n        (DataFrame): All embeddings as a `DataFrame[word: string, vector: vector]`.\n    \"\"\"\n    if not hasattr(self, \"node2vec_\"):\n        return\n\n    return self.node2vec_.getVectors()\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.SparkNode2Vec.fit","title":"<code>fit(G)</code>","text":"<p>Train the embedding by doing random walks.</p> <p>Random walk paths are available in attribute <code>paths_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>G</code> <code>Graph</code> <p>A <code>mercury.graph</code> Graph object. The embedding will be created so that each row in the embedding maps a node ID in G. (This parameter will be ignored when <code>load_file</code> is used.)</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted self (or raises an error)</p> Source code in <code>mercury/graph/embeddings/spark_node2vec.py</code> <pre><code>def fit(self, G: Graph):\n    \"\"\"\n    Train the embedding by doing random walks.\n\n    Random walk paths are available in attribute `paths_`.\n\n    Args:\n        G (mercury.graph.core.Graph): A `mercury.graph` Graph object. The embedding will be created so that each row in the embedding maps\n            a node ID in G. (This parameter will be ignored when `load_file` is used.)\n\n    Returns:\n        (self): Fitted self (or raises an error)\n    \"\"\"\n\n    if self.path_cache is None:\n        if self.use_cached_rw:\n            logging.warning(\n                \"Wrong options (use_cached_rw and no path_cache). \"\n                \"Paths will be recomputed.\"\n            )\n        self.use_cached_rw = False\n\n    if not self.use_cached_rw:\n        paths = (\n            self._run_rw(G)\n            .withColumn(\"size\", f.size(\"random_walks\"))\n            .where(f.col(\"size\") &gt; 1)\n            .drop(\"size\")\n        )\n\n        if self.path_cache is not None:\n            (\n                paths.repartition(self.n_partitions_cache)\n                .write.mode(\"overwrite\")\n                .parquet(\"%s/block=0\" % self.path_cache)\n            )\n\n        if self.num_paths_per_node &gt; 1:\n            for block_id in range(1, self.num_paths_per_node):\n                new_paths = (\n                    self._run_rw(G)\n                    .withColumn(\"size\", f.size(\"random_walks\"))\n                    .where(f.col(\"size\") &gt; 1)\n                    .drop(\"size\")\n                )\n                if self.path_cache is None:\n                    paths = paths.unionByName(new_paths)\n                else:\n                    (\n                        new_paths.repartition(self.n_partitions_cache)\n                        .write.mode(\"overwrite\")\n                        .parquet(\"%s/block=%d\" % (self.path_cache, block_id))\n                    )\n                    # With this, we clear the persisted dataframe\n                    new_paths.unpersist()\n\n    if self.path_cache is None:\n        self.paths_ = paths.persist()\n    else:\n        self.paths_ = (\n            SparkInterface()\n            .read_parquet(self.path_cache)\n            .drop(\"block\")\n            .repartition(self.n_partitions_cache)\n            .persist()\n        )\n\n    w2v = Word2Vec(\n        vectorSize=self.dimension,\n        maxIter=self.w2v_max_iter,\n        numPartitions=self.w2v_num_partitions,\n        stepSize=self.w2v_step_size,\n        inputCol=\"random_walks\",\n        outputCol=\"model\",\n        minCount=self.w2v_min_count,\n    )\n\n    self.node2vec_ = w2v.fit(self.paths_)\n\n    return self\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.SparkNode2Vec.get_most_similar_nodes","title":"<code>get_most_similar_nodes(node_id, k=5)</code>","text":"<p>Returns the k most similar nodes and a similarity measure.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>Id of the node we want to search.</p> required <code>k</code> <code>int</code> <p>Number of most similar nodes to return</p> <code>5</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A list of k most similar nodes (using cosine similarity) as a <code>DataFrame[word: string, similarity: double]</code></p> Source code in <code>mercury/graph/embeddings/spark_node2vec.py</code> <pre><code>def get_most_similar_nodes(self, node_id, k=5):\n    \"\"\"\n    Returns the k most similar nodes and a similarity measure.\n\n    Args:\n        node_id (str): Id of the node we want to search.\n        k (int): Number of most similar nodes to return\n\n    Returns:\n        (DataFrame): A list of k most similar nodes (using cosine similarity) as a `DataFrame[word: string, similarity: double]`\n    \"\"\"\n    if not hasattr(self, \"node2vec_\"):\n        return\n\n    return self.node2vec_.findSynonyms(node_id, k)\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.SparkNode2Vec.model","title":"<code>model()</code>","text":"<p>Returns the Spark Word2VecModel object.</p> <p>Returns:</p> Type Description <code>Word2VecModel</code> <p>The Spark Word2VecModel of the embedding to use its API directly.</p> Source code in <code>mercury/graph/embeddings/spark_node2vec.py</code> <pre><code>def model(self):\n    \"\"\"\n    Returns the Spark Word2VecModel object.\n\n    Returns:\n        (pyspark.ml.feature.Word2VecModel): The Spark Word2VecModel of the embedding to use its API directly.\n    \"\"\"\n    if not hasattr(self, \"node2vec_\"):\n        return\n\n    return self.node2vec_\n</code></pre>"},{"location":"reference/embeddings/#mercury.graph.embeddings.SparkNode2Vec.save","title":"<code>save(file_name)</code>","text":"<p>Saves the internal Word2VecModel to a human-readable (JSON) model metadata as a Parquet formatted data file.</p> <p>The model may be loaded using SparkNode2Vec(load_file='path/file')</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to which the Word2VecModel will be saved.</p> required Source code in <code>mercury/graph/embeddings/spark_node2vec.py</code> <pre><code>def save(self, file_name):\n    \"\"\"\n    Saves the internal Word2VecModel to a human-readable (JSON) model metadata as a Parquet formatted data file.\n\n    The model may be loaded using SparkNode2Vec(load_file='path/file')\n\n    Args:\n        file_name (str): The name of the file to which the Word2VecModel will be saved.\n    \"\"\"\n    if not hasattr(self, \"node2vec_\"):\n        return\n\n    return self.node2vec_.save(file_name)\n</code></pre>"},{"location":"reference/ml/","title":"mercury.graph.ml","text":""},{"location":"reference/ml/#mercury.graph.ml.LouvainCommunities","title":"<code>mercury.graph.ml.LouvainCommunities(min_modularity_gain=0.001, max_pass=2, max_iter=10, resolution=1, all_partitions=True, verbose=True)</code>","text":"<p>               Bases: <code>BaseClass</code></p> <p>Class that defines the functions that run a PySpark implementation of the Louvain algorithm to find the partition that maximizes the modularity of an undirected graph (as in <sup>1</sup>).</p> <p>This version of the algorithm differs from <sup>1</sup> in that the reassignment of nodes to new communities is calculated in parallel, not sequentially. That is, all nodes are reassigned at the same time and conflicts (i.e., 1 -&gt; C2 and 2 -&gt; C1) are resolved with a simple tie-breaking rule. This version also introduces the resolution parameter gamma, as in <sup>2</sup>.</p> <p>Contributed by Arturo Soberon Cedillo, Jose Antonio Guzman Vazquez and Isaac Dodanim Hernandez Garcia.</p> <ol> <li> <p>Blondel V D, Guillaume J-L, Lambiotte R and Lefebvre E (2008). Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008. https://doi.org/10.1088/1742-5468/2008/10/p10008 \u21a9\u21a9</p> </li> <li> <p>Aynaud T, Blondel V D, Guillaume J-L and Lambiotte R (2013). Multilevel local optimization of modularity. Graph Partitioning (315--345), 2013.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>min_modularity_gain</code> <code>float</code> <p>Modularity gain threshold between each pass. The algorithm stops if the gain in modularity between the current pass and the previous one is less than the given threshold.</p> <code>0.001</code> <code>max_pass</code> <code>int</code> <p>Maximum number of passes.</p> <code>2</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations within each pass.</p> <code>10</code> <code>resolution</code> <code>float</code> <p>The resolution parameter gamma. Its value must be greater or equal to zero. If resolution is less than 1, modularity favors larger communities, while values greater than 1 favor smaller communities.</p> <code>1</code> <code>all_partitions</code> <code>bool</code> <p>If True, the function will return all the partitions found at each step of the algorithm (i.e., pass0, pass1, pass2, ..., pass20). If False, only the last (and best) partition will be returned.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>If True, print progress information during the Louvain algorithm execution. Defaults to True.</p> <code>True</code> Source code in <code>mercury/graph/ml/louvain.py</code> <pre><code>def __init__(\n    self,\n    min_modularity_gain=1e-03,\n    max_pass=2,\n    max_iter=10,\n    resolution: Union[float, int] = 1,\n    all_partitions=True,\n    verbose=True,\n):\n    self.min_modularity_gain = min_modularity_gain\n    self.max_pass = max_pass\n    self.max_iter = max_iter\n    self.resolution = resolution\n    self.all_partitions = all_partitions\n    self.verbose = verbose\n\n    # Check resolution\n    if resolution &lt; 0:\n        exceptionMsg = f\"Resolution value is {resolution} and cannot be &lt; 0.\"\n        raise ValueError(exceptionMsg)\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.LouvainCommunities._calculate_m","title":"<code>_calculate_m(edges)</code>","text":"<p>Get the weighted size of an undirected graph (where \\(m\\) is defined as \\(m = \\frac{1}{2} \\sum_{ij} A_{ij}\\))).</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>DataFrame</code> <p>A pyspark dataframe representing the edges of an undirected graph. It must have <code>src</code> and <code>dst</code> as its columns. The user may also specify the weight of each edge via the additional <code>weight</code> column (optional).</p> required <p>Returns:</p> Type Description <code>int</code> <p>Returns the weighted size of the graph.</p> Source code in <code>mercury/graph/ml/louvain.py</code> <pre><code>def _calculate_m(self, edges) -&gt; int:\n    \"\"\"Get the weighted size of an undirected graph (where $m$ is\n    defined as $m = \\\\frac{1}{2} \\\\sum_{ij} A_{ij}$)).\n\n    Args:\n        edges (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the edges of an undirected graph.\n            It must have `src` and `dst` as its columns. The user may also\n            specify the weight of each edge via the additional `weight` column\n            (optional).\n\n    Returns:\n        (int): Returns the weighted size of the graph.\n    \"\"\"\n\n    m = edges.select(F.sum(\"weight\")).collect()[0][0]\n\n    return int(m)\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.LouvainCommunities._calculate_modularity","title":"<code>_calculate_modularity(edges, partition, m=None)</code>","text":"<p>This function calculates the modularity of a partition.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>DataFrame</code> <p>A pyspark dataframe representing the edges of an undirected graph. It must have <code>src</code> and <code>dst</code> as its columns. The user may also specify the weight of each edge via the additional <code>weight</code> column (optional).</p> required <code>partition</code> <code>DataFrame</code> <p>A pyspark dataframe representing the partition of an undirected graph (i.e., a table that indicates the community that each node belongs to). The dataframe must have columns <code>id</code> (indicating each node's ID) and <code>c</code> (indicating each node's assigned community).</p> required <code>m</code> <code>int</code> <p>The weighted size of the graph (the output of <code>_get_m()</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Bound between -1 and 1 representing the modularity of a partition. The output may exceed these bounds depending on the value of <code>resolution</code> (which is set to 1 by default).</p> Source code in <code>mercury/graph/ml/louvain.py</code> <pre><code>def _calculate_modularity(self, edges, partition, m=None) -&gt; float:\n    \"\"\"This function calculates the modularity of a partition.\n\n    Args:\n        edges (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the edges of an undirected graph.\n            It must have `src` and `dst` as its columns. The user may also\n            specify the weight of each edge via the additional `weight` column\n            (optional).\n\n        partition (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the partition of an undirected\n            graph (i.e., a table that indicates the community that each node\n            belongs to). The dataframe must have columns `id` (indicating each\n            node's ID) and `c` (indicating each node's assigned community).\n\n        m (int):\n            The weighted size of the graph (the output of `_get_m()`).\n\n    Returns:\n        (float):\n            Bound between -1 and 1 representing the modularity of a\n            partition. The output may exceed these bounds depending on the value of\n            `resolution` (which is set to 1 by default).\n    \"\"\"\n\n    # Calculate m (if necessary) and norm\n    m = self._calculate_m(edges) if m is None else m\n    norm = 1 / (2 * m)\n\n    # Declare basic inputs\n    labeledEdges = self._label_edges(edges, partition)\n    labeledDegrees = self._label_degrees(edges, partition)\n\n    # Get term on LHS\n    k_in = (labeledEdges.where(\"cSrc = cDst\").select(F.sum(\"weight\"))).collect()[0][\n        0\n    ]\n\n    # Handle NoneType\n    k_in = 0 if k_in is None else k_in\n\n    # Get term on RHS\n    k_out = (\n        labeledDegrees.groupby(\"c\")\n        .agg(F.sum(\"degree\").alias(\"kC\"))\n        .selectExpr(f\"{self.resolution} * sum(kC * kC)\")\n    ).collect()[0][0]\n\n    # Return modularity\n    return (k_in / m) - (norm**2 * float(k_out))\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.LouvainCommunities._label_degrees","title":"<code>_label_degrees(edges, partition)</code>","text":"<p>This function uses the edges of a graph to calculate the weighted degrees of each node and joins the result with the partition passed by the user.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>DataFrame</code> <p>A pyspark dataframe representing the edges of an undirected graph. It must have <code>src</code> and <code>dst</code> as its columns. The user may also specify the weight of each edge via the additional <code>weight</code> column (optional).</p> required <code>partition</code> <code>DataFrame</code> <p>A pyspark dataframe representing the partition of an undirected graph (i.e., a table that indicates the community that each node belongs to). The dataframe must have columns <code>id</code> (indicating each node's ID) and <code>c</code> (indicating each node's assigned community).</p> required <p>Returns:</p> Type Description <code>Dataframe</code> <p>This function returns a dataframe with columns <code>id</code> (representing the ID of each node in the graph), <code>c</code> (representing each node's community) and <code>degree</code> (representing each node's degree).</p> Source code in <code>mercury/graph/ml/louvain.py</code> <pre><code>def _label_degrees(self, edges, partition):\n    \"\"\"\n    This function uses the edges of a graph to calculate the weighted degrees\n    of each node and joins the result with the partition passed by the user.\n\n    Args:\n        edges (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the edges of an undirected graph.\n            It must have `src` and `dst` as its columns. The user may also\n            specify the weight of each edge via the additional `weight` column\n            (optional).\n\n        partition (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the partition of an undirected\n            graph (i.e., a table that indicates the community that each node\n            belongs to). The dataframe must have columns `id` (indicating each\n            node's ID) and `c` (indicating each node's assigned community).\n\n    Returns:\n        (Dataframe):\n            This function returns a dataframe with columns `id` (representing the ID\n            of each node in the graph), `c` (representing each node's community) and\n            `degree` (representing each node's degree).\n    \"\"\"\n\n    # Get id, community and weighted degree\n    ret = (\n        partition.join(\n            # Unite sources and destinations to avoid double join\n            other=(\n                edges.selectExpr(\"src as id\", \"weight\")\n                .unionByName(edges.selectExpr(\"dst as id\", \"weight\"))\n                .groupBy(\"id\")\n                .agg(F.sum(\"weight\").alias(\"degree\"))\n            ),\n            on=\"id\",\n            how=\"inner\",\n        )\n        .select(\"id\", \"c\", \"degree\")\n        .checkpoint()\n    )\n\n    return ret\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.LouvainCommunities._label_edges","title":"<code>_label_edges(edges, partition)</code>","text":"<p>This function uses <code>partition</code> to add two columns to <code>edges</code>. The added columns <code>cSrc</code> and <code>cDst</code> indicate the community that the source and destination nodes belong to.</p> <pre><code>Args:\nedges (pyspark.sql.dataframe.DataFrame):\n    A pyspark dataframe representing the edges of an undirected graph.\n    It must have `src` and `dst` as its columns. The user may also\n    specify the weight of each edge via the additional `weight` column\n    (optional).\n\npartition (pyspark.sql.dataframe.DataFrame):\n    A pyspark dataframe representing the partition of an undirected\n    graph (i.e., a table that indicates the community that each node\n    belongs to). The dataframe must have columns `id` (indicating each\n    node's ID) and `c` (indicating each node's assigned community).\n</code></pre> <p>Returns:</p> Type Description <code>DataFrame</code> <p>This function returns <code>edges</code> with two additional columns: the community that the source node belongs to (<code>cSrc</code>) and the community that the destination node belongs to (<code>cDst</code>).</p> Source code in <code>mercury/graph/ml/louvain.py</code> <pre><code>def _label_edges(self, edges, partition):\n    \"\"\"This function uses `partition` to add two columns to `edges`. The added\n    columns `cSrc` and `cDst` indicate the community that the source and\n    destination nodes belong to.\n\n        Args:\n        edges (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the edges of an undirected graph.\n            It must have `src` and `dst` as its columns. The user may also\n            specify the weight of each edge via the additional `weight` column\n            (optional).\n\n        partition (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the partition of an undirected\n            graph (i.e., a table that indicates the community that each node\n            belongs to). The dataframe must have columns `id` (indicating each\n            node's ID) and `c` (indicating each node's assigned community).\n\n    Returns:\n        (pyspark.sql.dataframe.DataFrame):\n            This function returns `edges` with two additional columns: the community\n            that the source node belongs to (`cSrc`) and the community that the\n            destination node belongs to (`cDst`).\n    \"\"\"\n\n    # Get communities\n    ret = (\n        edges\n        # Start off with src, dst and weight\n        .select(\"src\", \"dst\", \"weight\")\n        # Source destination\n        .join(\n            other=partition.selectExpr(\"id as src\", \"c as cSrc\"),\n            on=\"src\",\n            how=\"left\",\n        )\n        # Destination community\n        .join(\n            other=partition.selectExpr(\"id as dst\", \"c as cDst\"),\n            on=\"dst\",\n            how=\"left\",\n        ).checkpoint()\n    )\n\n    return ret\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.LouvainCommunities._last_pass","title":"<code>_last_pass(df)</code>","text":"<p>Returns the column name of the last pass.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pyspark dataframe representing the series of partitions made by <code>LouvainCommunities</code> (a dataframe with columns 'id', 'pass0', 'pass1', 'pass2', 'pass3', etc.).</p> required Source code in <code>mercury/graph/ml/louvain.py</code> <pre><code>def _last_pass(self, df):\n    \"\"\"Returns the column name of the last pass.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the series of partitions made by\n            `LouvainCommunities` (a dataframe with columns 'id', 'pass0',\n            'pass1', 'pass2', 'pass3', etc.).\n    \"\"\"\n\n    # Get all `passX` columns as list\n    cols = [col for col in df.columns if \"pass\" in col]\n\n    # Get last pass as int\n    _max = max([int(col.split(\"pass\")[1]) for col in cols])\n\n    # Return last pass as string\n    return f\"pass{_max}\"\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.LouvainCommunities._reassign_all","title":"<code>_reassign_all(edges, partition, m=None)</code>","text":"<p>This function simultaneously reassigns all the nodes in a graph to their corresponding optimal neighboring communities.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>DataFrame</code> <p>A pyspark dataframe representing the edges of an undirected graph. It must have <code>src</code> and <code>dst</code> as its columns. The user may also specify the weight of each edge via the additional <code>weight</code> column (optional).</p> required <code>partition</code> <code>DataFrame</code> <p>A pyspark dataframe representing the partition of an undirected graph (i.e., a table that indicates the community that each node belongs to). The dataframe must have columns <code>id</code> (indicating each node's ID) and <code>c</code> (indicating each node's assigned community).</p> required <code>m</code> <code>int</code> <p>The weighted size of the graph (the output of <code>getM()</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pyspark dataframe with the same number of rows as there are vertices. Columns <code>cx</code> and <code>cj</code> represent each node's current and optimal neighboring community (accordingly).</p> Source code in <code>mercury/graph/ml/louvain.py</code> <pre><code>def _reassign_all(self, edges, partition, m=None):\n    \"\"\"This function simultaneously reassigns all the nodes in a graph to their\n    corresponding optimal neighboring communities.\n\n    Args:\n        edges (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the edges of an undirected graph.\n            It must have `src` and `dst` as its columns. The user may also\n            specify the weight of each edge via the additional `weight` column\n            (optional).\n\n        partition (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the partition of an undirected\n            graph (i.e., a table that indicates the community that each node\n            belongs to). The dataframe must have columns `id` (indicating each\n            node's ID) and `c` (indicating each node's assigned community).\n\n        m (int):\n            The weighted size of the graph (the output of `getM()`).\n\n    Returns:\n        (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe with the same number of rows as there are vertices.\n            Columns `cx` and `cj` represent each node's current and optimal\n            neighboring community (accordingly).\n    \"\"\"\n\n    # Calculate m if necessary\n    m = self._calculate_m(edges) if m is None else m\n\n    # Label edges and degrees here to avoid long lineages\n    labeledDegrees = self._label_degrees(edges, partition)\n    labeledEdges = self._label_edges(edges, partition)\n\n    # Add sum(ki) for i in C to labeledDegrees\n    dq = (\n        labeledDegrees.withColumn(\n            \"cx_sum_ki\", F.sum(\"degree\").over(Window.partitionBy(\"c\"))\n        )\n        # Get sum(Aix) for i in Cx\\{x}\n        .join(\n            other=(\n                labeledEdges.where(\"(src != dst) and (cSrc = cDst)\")\n                .selectExpr(\"src as id\", \"weight\")\n                .unionByName(\n                    labeledEdges.where(\"(src != dst) and (cSrc = cDst)\").selectExpr(\n                        \"dst as id\", \"weight\"\n                    )\n                )\n                .groupBy(\"id\")\n                .agg(F.sum(\"weight\").alias(\"cx_sum_aix\"))\n            ),\n            on=\"id\",\n            how=\"left\",\n        )\n        # Get sum(Aix) for i in Cj (relationship 1:J)\n        .join(\n            other=(\n                labeledEdges.where(\"cSrc != cDst\")\n                .selectExpr(\"src as id\", \"cDst as cj\", \"weight\")\n                .unionByName(\n                    labeledEdges.where(\"cSrc != cDst\").selectExpr(\n                        \"dst as id\", \"cSrc as cj\", \"weight\"\n                    )\n                )\n                .groupBy(\"id\", \"cj\")\n                .agg(F.sum(\"weight\").alias(\"cj_sum_aix\"))\n            ),\n            on=\"id\",\n            how=\"left\",\n        )\n        # Get sum(ki) for i in Cj\n        .join(\n            other=(\n                labeledDegrees.withColumnRenamed(\"c\", \"cj\")\n                .groupBy(\"cj\")\n                .agg(F.sum(\"degree\").alias(\"cj_sum_ki\"))\n            ),\n            on=\"cj\",\n            how=\"left\",\n        )\n        # Calculate modularity change of each possible switch (Cx -&gt; {x} -&gt; Cj)\n        .withColumn(\n            \"mdq\",\n            F.coalesce(\"cj_sum_aix\", F.lit(0))\n            - F.coalesce(\"cx_sum_aix\", F.lit(0))\n            - (\n                F.col(\"degree\")\n                / F.lit(2 * m)\n                * (F.col(\"cj_sum_ki\") - F.col(\"cx_sum_ki\") + F.col(\"degree\"))\n            ),\n        )\n        # Rank mdq(x) in descending order\n        .select(\n            F.col(\"id\"),\n            F.col(\"c\"),\n            F.coalesce(\"cj\", F.col(\"c\")).alias(\"cj\"),  # Trapped nodes: Cx == Cj\n            F.col(\"mdq\"),\n            F.row_number()\n            .over(Window.partitionBy(\"id\").orderBy(F.desc(\"mdq\")))\n            .alias(\"mdq_rank\"),\n        )\n        # Keep best (or first) change\n        .where(F.col(\"mdq_rank\") == 1)\n    )\n\n    # Break symmetric swaps (only in first iteration?)\n    dq = (\n        dq.withColumn(\n            \"sym_rank\",\n            F.row_number().over(\n                Window.partitionBy(\n                    F.sort_array(F.array(F.col(\"c\"), F.col(\"cj\")))\n                ).orderBy(F.desc(\"mdq\"))\n            ),\n        )\n        # Select best switch (cStar) and break symmetric swaps with sym_rank\n        .withColumn(\n            \"cStar\",\n            F.when(\n                ((F.col(\"mdq\") &gt; F.lit(1e-04)) &amp; (F.col(\"sym_rank\") == 1)),\n                F.col(\"cj\"),\n            ).otherwise(F.col(\"c\")),\n        ).selectExpr(\"id\", \"c as cx\", \"cStar as cj\")\n    )\n\n    return dq\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.LouvainCommunities._sort_passes","title":"<code>_sort_passes(res)</code>","text":"<p>Takes the output of <code>LouvainCommunities</code> and returns a list containing its columns ordered by their integer part in ascending order. For example, if the columns returned by <code>LouvainCommunities are</code>['pass2', 'id', 'pass1', 'pass0']<code>, this function will turn the list to</code>['id', 'pass0', 'pass1', 'pass2']<code>. This function also supports cases where</code>max_pass &gt; 10`.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>DataFrame</code> <p>A pyspark dataframe representing the output of <code>LouvainCommunities</code>. <code>res</code> must have columns 'id', 'pass0', 'pass1', 'pass2', etc.</p> required Source code in <code>mercury/graph/ml/louvain.py</code> <pre><code>def _sort_passes(self, res) -&gt; list:\n    \"\"\"Takes the output of `LouvainCommunities` and returns a list containing\n    its columns ordered by their integer part in ascending order.\n    For example, if the columns returned by `LouvainCommunities are\n    `['pass2', 'id', 'pass1', 'pass0']`, this function will turn the list to\n    `['id', 'pass0', 'pass1', 'pass2']`.\n    This function also supports cases where `max_pass &gt; 10`.\n\n    Args:\n        res (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the output of `LouvainCommunities`.\n            `res` must have columns 'id', 'pass0', 'pass1', 'pass2', etc.\n    \"\"\"\n\n    # Get pass-columns and sort them by their integer part\n    cols = [col for col in res.columns if \"pass\" in col]\n    ints = sorted([int(col.replace(\"pass\", \"\")) for col in cols])\n    cols_sorted = [\"id\"] + [\"pass\" + str(i) for i in ints]\n\n    return cols_sorted\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.LouvainCommunities._verify_data","title":"<code>_verify_data(df, expected_cols_grouping, expected_cols_others)</code>","text":"<p>Checks if <code>edges</code> meets the format expected by <code>LouvainCommunities</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pyspark dataframe representing the edges of an undirected graph. It must have <code>src</code> and <code>dst</code> as its columns. The user may also specify the weight of each edge via the additional <code>weight</code> column (optional).</p> required <code>expected_cols_grouping</code> <code>list</code> <p>A list of strings representing the columns that must be present in <code>df</code> to group the data.</p> required <code>expected_cols_others</code> <code>list</code> <p>A list of strings representing the columns that must be present in <code>df</code> but are not used for grouping.</p> required Source code in <code>mercury/graph/ml/louvain.py</code> <pre><code>def _verify_data(self, df, expected_cols_grouping, expected_cols_others):\n    \"\"\"Checks if `edges` meets the format expected by `LouvainCommunities`.\n\n    Args:\n        df (pyspark.sql.dataframe.DataFrame):\n            A pyspark dataframe representing the edges of an undirected graph.\n            It must have `src` and `dst` as its columns. The user may also\n            specify the weight of each edge via the additional `weight` column\n            (optional).\n\n        expected_cols_grouping (list):\n            A list of strings representing the columns that must be present in\n            `df` to group the data.\n\n        expected_cols_others (list):\n            A list of strings representing the columns that must be present in\n            `df` but are not used for grouping.\n    \"\"\"\n\n    cols = df.columns\n    expected_cols = expected_cols_grouping + expected_cols_others\n\n    # Check type\n    if not isinstance(df, DataFrame):\n        raise TypeError(\"Input data must be a pyspark DataFrame.\")\n\n    # Check missing columns\n    msg = \"Input data is missing expected column '{}'.\"\n    for col in expected_cols:\n        if col not in cols:\n            raise ValueError(msg.format(col))\n\n    # Check for duplicates\n    dup = (\n        df.groupBy(*expected_cols_grouping)\n        .agg(F.count(F.lit(1)).alias(\"count\"))\n        .where(\"count &gt; 1\")\n        .count()\n    )\n    if dup &gt; 0:\n        raise ValueError(\"Data has duplicated entries.\")\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.LouvainCommunities.fit","title":"<code>fit(g)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>g</code> <code>Graph</code> <p>A mercury graph structure.</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted self (or raises an error).</p> Source code in <code>mercury/graph/ml/louvain.py</code> <pre><code>def fit(self, g: Graph):\n    \"\"\"\n    Args:\n        g (Graph): A mercury graph structure.\n\n    Returns:\n        (self): Fitted self (or raises an error).\n    \"\"\"\n    edges = g.graphframe.edges\n\n    # Verify edges input\n    self._verify_data(\n        df=edges,\n        expected_cols_grouping=[\"src\", \"dst\"],\n        expected_cols_others=[\"weight\"],\n    )\n\n    # Init dataframe to be returned\n    ret = (\n        edges.selectExpr(\"src as id\")\n        .unionByName(edges.selectExpr(\"dst as id\"))\n        .distinct()\n        .withColumn(\"pass0\", F.row_number().over(Window.orderBy(\"id\")))\n    ).checkpoint()\n\n    # Convert edges to anonymized src's and dst's\n    edges = (\n        edges.selectExpr(\"src as src0\", \"dst as dst0\", \"weight\")\n        .join(other=ret.selectExpr(\"id as src0\", \"pass0 as src\"), on=\"src0\")\n        .join(other=ret.selectExpr(\"id as dst0\", \"pass0 as dst\"), on=\"dst0\")\n        .select(\"src\", \"dst\", \"weight\")\n    ).checkpoint()\n\n    # Calculate m and initialize modularity\n    m = self._calculate_m(edges)\n    modularity0 = -1.0\n\n    # Begin pass\n    canPass, _pass = True, 0\n    while canPass:\n\n        # Declare naive partition\n        p1 = (\n            edges.selectExpr(\"src as id\")\n            .unionByName(edges.selectExpr(\"dst as id\"))\n            .distinct()\n            .withColumn(\"c\", F.col(\"id\"))\n        )\n\n        # Begin iterations within pass\n        canIter, _iter = True, 0\n        # Carry reference to previously cached p2 to call unpersist()\n        prev_p2 = None\n        while canIter:\n\n            if _iter &gt;= self.max_iter:\n                break\n\n            # Print progress\n            if self.verbose:\n                print(f\"Starting Pass {_pass} Iteration {_iter}.\")\n\n            # Create new partition and check if movements were made\n            p2 = self._reassign_all(edges, p1)\n            # Break complex lineage caused by loops first\n            p2 = p2.checkpoint()\n            p2.cache()\n\n            canIter = len(p2.where(\"cx != cj\").take(1)) &gt; 0\n            if canIter:\n                p1 = p2.selectExpr(\"id\", \"cj as c\")\n            if prev_p2 is not None:\n                prev_p2.unpersist()\n            prev_p2 = p2\n            _iter += 1\n\n        # Calculate new modularity and update pass counter\n        modularity1 = self._calculate_modularity(edges=edges, partition=p1, m=m)\n\n        # Declare stopping criterion and update old modularity\n        canPass = (modularity1 - modularity0 &gt; self.min_modularity_gain) and (\n            _pass &lt; self.max_pass\n        )\n        modularity0 = modularity1\n\n        self.modularity_ = modularity0\n\n        # Update ret and compress graph\n        if canPass:\n            ret = ret.join(\n                other=p1.selectExpr(f\"id as pass{_pass}\", f\"c as pass{_pass + 1}\"),\n                on=f\"pass{_pass}\",\n            ).checkpoint()\n\n            edges = (\n                self._label_edges(edges, p1)\n                .select(\"cSrc\", \"cDst\", \"weight\")\n                .groupBy(\"cSrc\", \"cDst\")\n                .agg(F.sum(\"weight\").alias(\"weight\"))\n                .selectExpr(\"cSrc as src\", \"cDst as dst\", \"weight\")\n            ).checkpoint()\n\n        prev_p2.unpersist()\n        _pass += 1\n\n    # Return final dataframe with sorted columns\n    if self.all_partitions:\n\n        # Return sorted columns\n        cols = self._sort_passes(ret)\n        ret = ret.select(cols)\n\n    # Return final dataframe with id &amp; community\n    else:\n        _last = self._last_pass(ret)\n        ret = ret.selectExpr(\"id as node_id\", f\"{_last} as cluster\")\n\n    self.labels_ = ret\n\n    return self\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SparkRandomWalker","title":"<code>mercury.graph.ml.SparkRandomWalker(num_epochs=10, batch_size=1, n_sampling_edges=None)</code>","text":"<p>               Bases: <code>BaseClass</code></p> <p>Class to perform random walks from a specific source_id node within a given Graph</p> <p>Parameters:</p> Name Type Description Default <code>num_epochs</code> <code>int</code> <p>Number of epochs. This is the total number of steps the iteration goes through.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>This forces caching the random walks computed so far and breaks planning each time this number of epochs is reached. The default value is a high number to avoid this entering at all. In really large jobs, you may want to set this parameter to avoid possible overflows even if it can add some extra time to the process. Note that with a high number of epochs and nodes resource requirements for the active part of your random walks can be high. This allows to \"cache a continue\" so to say.</p> <code>1</code> <code>n_sampling_edges</code> <code>int</code> <p>by setting this parameter you can limit at each timestep the number of new paths opened from each node. This is useful when the graph contains nodes with very high out-degree, where running the algorithm several epochs is not feasible. When using this parameter, the graph will consider only at most <code>edge_sampling</code> outgoing edges at each epoch for each path. If the last node of the path contains more than <code>edge_sampling</code> the selected edges are sampled using its weight.</p> <code>None</code> Source code in <code>mercury/graph/ml/spark_randomwalker.py</code> <pre><code>def __init__(self, num_epochs=10, batch_size=1, n_sampling_edges=None):\n    \"\"\"\n    Class to perform random walks from a specific source_id node within a given Graph\n\n    Args:\n        num_epochs (int): Number of epochs. This is the total number of steps the iteration goes through.\n        batch_size (int): This forces caching the random walks computed so far and breaks planning each time this number of epochs\n            is reached. The default value is a high number to avoid this entering at all. In really large jobs, you may want to\n            set this parameter to avoid possible overflows even if it can add some extra time to the process. Note that with a high\n            number of epochs and nodes resource requirements for the active part of your random walks can be high. This allows to\n            \"cache a continue\" so to say.\n        n_sampling_edges (int): by setting this parameter you can limit at each timestep the number of new paths opened from each node.\n            This is useful when the graph contains nodes with very high out-degree, where running the algorithm several epochs is\n            not feasible. When using this parameter, the graph will consider only at most `edge_sampling` outgoing edges at each\n            epoch for each path. If the last node of the path contains more than `edge_sampling` the selected edges are sampled\n            using its weight.\n    \"\"\"\n    self.num_epochs = num_epochs\n    self.batch_size = batch_size\n    self.n_sampling_edges = n_sampling_edges\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SparkRandomWalker.fit","title":"<code>fit(G, source_id)</code>","text":"<p>Perform random walks from a specific source_id node within a given Graph</p> <p>Parameters:</p> Name Type Description Default <code>G</code> <code>mercury.graph Graph asset</code> <p>A <code>mercury.graph</code> Graph</p> required <code>source_id</code> <code>int / str / list</code> <p>the source vertex or list for vertices to start the random walks.</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted self (or raises an error)</p> <p>Attribute <code>paths_</code> contains a Spark Dataframe with a columns <code>random_walks</code> containing an array of the elements of the path walked and another column with the corresponding weights. The weights represent the probability of following that specific path starting from source_id.</p> Source code in <code>mercury/graph/ml/spark_randomwalker.py</code> <pre><code>def fit(self, G: Graph, source_id):\n    \"\"\"\n    Perform random walks from a specific source_id node within a given Graph\n\n    Args:\n        G (mercury.graph Graph asset): A `mercury.graph` Graph\n        source_id (int/str/list): the source vertex or list for vertices to start the random walks.\n\n    Returns:\n        (self): Fitted self (or raises an error)\n\n    Attribute `paths_` contains a Spark Dataframe with a columns `random_walks` containing an array of the elements\n    of the path walked and another column with the corresponding weights. The weights represent the probability of\n    following that specific path starting from source_id.\n    \"\"\"\n    self.paths_ = self._run_rw(G, source_id)\n\n    return self\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SparkSpreadingActivation","title":"<code>mercury.graph.ml.SparkSpreadingActivation(attribute='influence', spreading_factor=0.2, transfer_function='weighted', steps=1, influenced_by=False)</code>","text":"<p>               Bases: <code>BaseClass</code></p> <p>This class is a model that represents a \u201cword-of-mouth\u201d scenario where a node influences his neighbors, from where the influence spreads to other neighbors, and so on.</p> <p>At the end of the diffusion process, we inspect the amount of influence received by each node. Using a threshold-based technique, a node that is currently not influenced can be declared to be a potential future one, based on the influence that has been accumulated.</p> <p>The diffusion model is based on Spreading Activation (SPA) techniques proposed in cognitive psychology and later used for trust metric computations. For more details, please see paper entitled \"Social Ties and their Relevance to Churn in Mobile Telecom Networks\"</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>Column name which will store the amount of influence spread</p> <code>'influence'</code> <code>spreading_factor</code> <code>float</code> <p>Percentage of influence to distribute. Low values favor influence proximity to the source of injection, while high values allow the influence to also reach nodes which are further away. It must be a value in the range (0,1). Default value is 0.2</p> <code>0.2</code> <code>transfer_function</code> <code>str</code> <p>Allowed values: \"weighted\" or \"unweighted\". Once a node decides what fraction of energy to distribute, the next step is to decide what fraction of the energy is transferred to each neighbor. This is controlled by the Transfer Function. If \"weighted\" then the energy distributed along the directed edge  depends on its relatively weight compared to the sum of weights of all outgoing edges of X. If \"unweighted\", then the energy distributed along the edge  is independent of its relatively weight. <code>'weighted'</code> <code>steps</code> <code>int</code> <p>Number of steps to perform</p> <code>1</code> <code>influenced_by</code> <code>bool</code> <p>if True, and extra column \"influenced_by\" is calculated which contains the seed nodes that have spread some influence to a given node. When True, the ids of the nodes cannot contain commas \",\". Note that seed_nodes will have at least their own (remaining) influence</p> <code>False</code> Source code in <code>mercury/graph/ml/spark_spreadactivation.py</code> <pre><code>def __init__(\n    self,\n    attribute: str = \"influence\",\n    spreading_factor: float = 0.2,\n    transfer_function: str = \"weighted\",\n    steps: int = 1,\n    influenced_by: bool = False,\n):\n    self.attribute = attribute\n    self.spreading_factor = spreading_factor\n    self.transfer_function = transfer_function\n    self.steps = steps\n    self.influenced_by = influenced_by\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SparkSpreadingActivation._compute_degrees","title":"<code>_compute_degrees(g)</code>","text":"<p>Compute weighted and unweighted in and out degrees in graph. Re-declares graph to add the following attributes: inDegree, outDegree, w_inDegree, w_outDegree.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Graph</code> <p>graphframe object, network</p> required Source code in <code>mercury/graph/ml/spark_spreadactivation.py</code> <pre><code>def _compute_degrees(self, g: Graph):\n    \"\"\"\n    Compute weighted and unweighted in and out degrees in graph. Re-declares graph to add the following\n    attributes: inDegree, outDegree, w_inDegree, w_outDegree.\n\n    Args:\n        g: graphframe object, network\n    \"\"\"\n    g_vertices = g.graphframe.vertices\n    g_edges = g.graphframe.edges\n\n    # Get unweighted degrees\n    indeg = g.graphframe.inDegrees\n    outdeg = g.graphframe.outDegrees\n\n    # Get weighted degrees\n    w_indeg = (\n        g_edges.groupby(\"dst\").agg(f.sum(\"weight\").alias(\"w_inDegree\"))\n    ).selectExpr(\"dst as id\", \"w_inDegree as w_inDegree\")\n    w_outdeg = (\n        g_edges.groupby(\"src\").agg(f.sum(\"weight\").alias(\"w_outDegree\"))\n    ).selectExpr(\"src as id\", \"w_outDegree as w_outDegree\")\n\n    # Update vertices attribute\n    new_v = g_vertices.join(indeg, \"id\", \"left_outer\")\n    new_v = new_v.join(outdeg, \"id\", \"left_outer\")\n    new_v = new_v.join(w_indeg, \"id\", \"left_outer\")\n    new_v = new_v.join(w_outdeg, \"id\", \"left_outer\")\n    new_v = new_v.na.fill(0)\n\n    # Update graph\n    return Graph(GraphFrame(new_v, g_edges))\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SparkSpreadingActivation._set_seed_nodes","title":"<code>_set_seed_nodes(g, seed_nodes=None)</code>","text":"<p>Set seed nodes which are the source of influence using pyspark dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Graph</code> <p>A <code>mercury.graph</code> Graph object.</p> required <code>seed_nodes</code> <code>Union[List, DataFrame]</code> <p>Collection of nodes that are the source to spread the influence. It must be pyspark dataframe with column 'id' or python list.</p> <code>None</code> Source code in <code>mercury/graph/ml/spark_spreadactivation.py</code> <pre><code>def _set_seed_nodes(\n    self,\n    g: Graph,\n    seed_nodes: Union[List, \"pyspark.sql.DataFrame\"] = None,\n):\n    \"\"\"\n    Set seed nodes which are the source of influence using pyspark dataframe.\n\n    Args:\n        g (mercury.graph.core.Graph): A `mercury.graph` Graph object.\n        seed_nodes (Union[List, pyspark.sql.DataFrame]): Collection of nodes that are the source to spread\n            the influence. It must be pyspark dataframe with column 'id' or python list.\n    \"\"\"\n\n    seed_nodes_dataframe = seed_nodes\n\n    # Convert list to dataframe\n    if isinstance(seed_nodes, list):\n        rdd_list = SparkInterface().spark.sparkContext.parallelize(seed_nodes)\n        row_rdd_list = rdd_list.map(lambda x: Row(x))\n        field_list = [StructField(\"id\", StringType(), True)]\n        schema_list = StructType(field_list)\n        seed_nodes_dataframe = SparkInterface().spark.createDataFrame(\n            row_rdd_list, schema_list\n        )\n\n    # Create column for influence attribute containing 1's\n    seed_nodes_dataframe = seed_nodes_dataframe.withColumn(\n        self.attribute, f.lit(1.0)\n    )\n    self.seed_nodes_ = seed_nodes_dataframe\n\n    # Merge to original vertices of graph\n    orig_vertices = g.graphframe.vertices.select(\"id\")\n    orig_edges = g.graphframe.edges\n    new_vertices = orig_vertices.join(\n        seed_nodes_dataframe, \"id\", \"left_outer\"\n    ).na.fill(0)\n\n    # If influenced_by flag is set, then initialize the seed nodes\n    if self.influenced_by:\n        new_vertices = new_vertices.withColumn(\n            \"influenced_by\",\n            f.when(\n                new_vertices[self.attribute] == 1,\n                f.split(new_vertices[\"id\"], pattern=\",\"),\n            ).otherwise(f.array().cast(\"array&lt;string&gt;\")),\n        )\n\n    # Update graph\n    return Graph(GraphFrame(new_vertices, orig_edges))\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SparkSpreadingActivation._spread_activation_step","title":"<code>_spread_activation_step(g)</code>","text":"<p>One step in the spread activation model.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Graph</code> <p>graphframe object, network</p> required <p>Returns:</p> Type Description <code>Graphframe</code> <p>new network with updated new calculation of attribute in vertices</p> Source code in <code>mercury/graph/ml/spark_spreadactivation.py</code> <pre><code>def _spread_activation_step(self, g: Graph):\n    \"\"\"\n    One step in the spread activation model.\n\n    Args:\n        g: graphframe object, network\n\n    Returns:\n        (Graphframe): new network with updated new calculation of attribute in vertices\n    \"\"\"\n\n    # Pass influence/message to neighboring nodes (weighted/unweighted option)\n    if self.transfer_function == \"unweighted\":\n        msg_to_src = (AM.src[self.attribute] / AM.src[\"outDegree\"]) * (\n            1 - self.spreading_factor\n        )\n        msg_to_dst = f.when(\n            AM.dst[\"outDegree\"] != 0,\n            (\n                (AM.src[self.attribute] / AM.src[\"outDegree\"])\n                * self.spreading_factor\n            ),\n        ).otherwise(\n            ((1 / AM.dst[\"inDegree\"]) * AM.dst[self.attribute])\n            + (\n                (AM.src[self.attribute] / AM.src[\"outDegree\"])\n                * self.spreading_factor\n            )\n        )\n\n    elif self.transfer_function == \"weighted\":\n        weight = AM.edge[\"weight\"] / AM.src[\"w_outDegree\"]\n        msg_to_src = (AM.src[self.attribute] / AM.src[\"outDegree\"]) * (\n            1 - self.spreading_factor\n        )\n        msg_to_dst = f.when(\n            AM.dst[\"outDegree\"] != 0,\n            ((AM.src[self.attribute]) * (self.spreading_factor * weight)),\n        ).otherwise(\n            ((1 / AM.dst[\"inDegree\"]) * AM.dst[self.attribute])\n            + ((AM.src[self.attribute]) * (self.spreading_factor * weight))\n        )\n\n    # Aggregate messages\n    agg = g.graphframe.aggregateMessages(\n        f.sum(AM.msg).alias(self.attribute),\n        sendToSrc=msg_to_src,\n        sendToDst=msg_to_dst,\n    )\n\n    # Create a new cached copy of the dataFrame to get new calculated attribute\n    cached_new_vertices = AM.getCachedDataFrame(agg)\n\n    if self.influenced_by:\n        to_join = g.graphframe.vertices.select(\n            \"id\",\n            \"inDegree\",\n            \"outDegree\",\n            \"w_inDegree\",\n            \"w_outDegree\",\n            \"influenced_by\",\n        )\n    else:\n        to_join = g.graphframe.vertices.select(\n            \"id\", \"inDegree\", \"outDegree\", \"w_inDegree\", \"w_outDegree\"\n        )\n    new_cached_new_vertices = cached_new_vertices.join(to_join, \"id\", \"left_outer\")\n    new_cached_new_vertices = new_cached_new_vertices.na.fill(0)\n\n    # If influenced_by flag is set, compute new seed nodes influencing\n    if self.influenced_by:\n        new_cached_new_vertices = self._calculate_influenced_by(\n            g, new_cached_new_vertices\n        )\n\n    # Return graph with new calculated attribute\n    return Graph(GraphFrame(new_cached_new_vertices, g.graphframe.edges))\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SparkSpreadingActivation.fit","title":"<code>fit(g, seed_nodes)</code>","text":"<p>Perform all iterations of spread_activation</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Graph</code> <p>A <code>mercury.graph</code> Graph object.</p> required <code>seed_nodes</code> <code>Union[List, DataFrame]</code> <p>Collection of nodes that are the \"seed\" or are the source to spread the influence. It must be pyspark dataframe with column 'id' or python list</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted self</p> Source code in <code>mercury/graph/ml/spark_spreadactivation.py</code> <pre><code>def fit(\n    self,\n    g: Graph,\n    seed_nodes: Union[List, \"pyspark.sql.DataFrame\"],\n):\n    \"\"\"\n    Perform all iterations of spread_activation\n\n    Args:\n        g (mercury.graph.core.Graph): A `mercury.graph` Graph object.\n        seed_nodes (Union[List, pyspark.sql.DataFrame]): Collection of nodes that are the \"seed\" or are the source to spread\n            the influence. It must be pyspark dataframe with column 'id' or python list\n\n    Returns:\n        (self): Fitted self\n    \"\"\"\n\n    # Set seed nodes which are the source of influence\n    g = self._set_seed_nodes(g, seed_nodes)\n\n    # Compute degrees\n    g = self._compute_degrees(g)\n\n    # Number of iterations specified for spread activation\n    for _ in range(0, self.steps, 1):\n        g = self._spread_activation_step(\n            g,\n        )\n\n    # Graph with updated attributes\n    self.fitted_graph_ = g\n    # Influences as DataFrame\n    self.influences_ = self.fitted_graph_.nodes_as_dataframe().select(\n        \"id\", \"influence\"\n    )\n\n    return self\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SpectralClustering","title":"<code>mercury.graph.ml.SpectralClustering(n_clusters=2, mode='networkx', max_iterations=10, random_state=0)</code>","text":"<p>               Bases: <code>BaseClass</code></p> <p>Implementation of the spectral clustering algorithm which detect communities inside a graph.</p> <p>Contributed by Gibran Gabriel Otazo Sanchez.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>The number of clusters that you want to detect.</p> <code>2</code> <code>random_state</code> <code>int</code> <p>Seed for reproducibility</p> <code>0</code> <code>mode</code> <code>str</code> <p>Calculation mode. Pass 'networkx' for using pandas + networkx or         'spark' for spark + graphframes</p> <code>'networkx'</code> <code>max_iterations</code> <code>int</code> <p>Max iterations parameter (only used if mode==spark)</p> <code>10</code> Source code in <code>mercury/graph/ml/spectral.py</code> <pre><code>def __init__(\n    self, n_clusters=2, mode=\"networkx\", max_iterations=10, random_state=0\n):\n    self.n_clusters = n_clusters\n    self.mode = mode\n    self.max_iterations = max_iterations\n    self.random_state = random_state\n\n    if self.mode not in (\"networkx\", \"spark\"):\n        raise ValueError(\"Error: Mode must be either 'networkx' or 'spark'\")\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SpectralClustering._fit_networkx","title":"<code>_fit_networkx(graph)</code>","text":"<p>Spectral clustering but using networkx (local mode implementation)</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>A mercury graph structure.</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted self (or raises an error)</p> Source code in <code>mercury/graph/ml/spectral.py</code> <pre><code>def _fit_networkx(self, graph: Graph):\n    \"\"\"\n    Spectral clustering but using networkx (local mode implementation)\n\n    Args:\n        graph (Graph): A mercury graph structure.\n\n    Returns:\n        (self): Fitted self (or raises an error)\n    \"\"\"\n    gnx = graph.networkx.to_undirected()\n\n    L = normalized_laplacian_matrix(gnx).todense()\n\n    if not np.allclose(L, L.T):\n        raise ValueError(\"Normalized Laplacian matrix of the undirected graph should be symmetric\")\n\n    w, v = eigh(L)\n\n    U = v[:, : self.n_clusters]\n    U = asarray(U)\n\n    kmeans = KMeans(\n        n_clusters=self.n_clusters, random_state=self.random_state, n_init=\"auto\"\n    ).fit(U)\n\n    self.labels_ = DataFrame({\"node_id\": gnx.nodes(), \"cluster\": kmeans.labels_})\n\n    cluster_nodes = self.labels_.groupby(\"cluster\")[\"node_id\"].apply(list)\n    self.modularity_ = nx_modularity(gnx, cluster_nodes)\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SpectralClustering._fit_spark","title":"<code>_fit_spark(graph)</code>","text":"<p>Spectral clustering but using pyspark</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>A mercury graph structure.</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted self (or raises an error)</p> Source code in <code>mercury/graph/ml/spectral.py</code> <pre><code>def _fit_spark(self, graph: Graph):\n    \"\"\"\n    Spectral clustering but using pyspark\n\n    Args:\n        graph (Graph): A mercury graph structure.\n\n    Returns:\n        (self): Fitted self (or raises an error)\n    \"\"\"\n\n    graph_frames_graph = graph.graphframe\n\n    pic = PowerIterationClustering(k=self.n_clusters, weightCol=\"weight\")\n    pic.setMaxIter(self.max_iterations)\n\n    # Node ids can be strings, with this we ensure IDs are always converted to\n    # integers (needed by PowerIterationClustering)\n    vertices_mapping = graph_frames_graph.vertices.withColumn(\n        \"idx\", F.monotonically_increasing_id()\n    )\n\n    mapped_node_ids = (\n        graph_frames_graph.edges.join(\n            vertices_mapping, graph_frames_graph.edges.src == vertices_mapping.id\n        )\n        .withColumnRenamed(\"idx\", \"src_mapped\")\n        .drop(\"id\", \"src\")\n    )\n\n    mapped_node_ids = (\n        mapped_node_ids.join(\n            vertices_mapping, mapped_node_ids.dst == vertices_mapping.id\n        )\n        .withColumnRenamed(\"idx\", \"dst_mapped\")\n        .drop(\"id\", \"dst\")\n        .withColumnRenamed(\"src_mapped\", \"src\")\n        .withColumnRenamed(\"dst_mapped\", \"dst\")\n    )\n    assignments = pic.assignClusters(mapped_node_ids)\n\n    self.labels_ = (\n        vertices_mapping.join(assignments, vertices_mapping.idx == assignments.id)\n        .drop(assignments.id)\n        .selectExpr([\"id as node_id\", \"cluster\"])\n    )\n\n    self.modularity_ = self._spark_modularity(\n        graph_frames_graph.edges, graph_frames_graph.degrees\n    )\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SpectralClustering._spark_modularity","title":"<code>_spark_modularity(edges, degrees, resolution=1)</code>","text":"<p>Computes modularity using the same approximation as networkx: https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.modularity.html</p> Source code in <code>mercury/graph/ml/spectral.py</code> <pre><code>def _spark_modularity(self, edges, degrees, resolution=1):\n    \"\"\"Computes modularity using the same approximation as networkx:\n    https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.modularity.html\n    \"\"\"\n\n    edge_nb = edges.count()\n    q = []\n\n    for i in range(self.n_clusters):\n        nids = self.labels_[self.labels_.cluster == i]\n        nodeids = [row[\"node_id\"] for row in nids.select(\"node_id\").collect()]\n\n        l_c = edges.filter(\n            edges.src.isin(nodeids) &amp; edges.dst.isin(nodeids)\n        ).count()\n\n        k_c = (\n            nids.join(degrees.withColumnRenamed(\"id\", \"node_id\"), on=\"node_id\")\n            .select(F.sum(\"degree\"))\n            .collect()[0][0]\n        )\n\n        qi = (l_c / edge_nb) - resolution * (k_c / (2 * edge_nb)) ** 2\n        q.append(qi)\n\n    return np.sum(q)\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.SpectralClustering.fit","title":"<code>fit(graph)</code>","text":"<p>Find the optimal clusters of a given graph. The function returns nothing, but saves the clusters and the modularity in the object self.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>A mercury graph structure.</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted self (or raises an error)</p> Source code in <code>mercury/graph/ml/spectral.py</code> <pre><code>def fit(self, graph: Graph):\n    \"\"\"\n    Find the optimal clusters of a given graph. The function returns nothing, but saves the clusters and\n    the modularity in the object self.\n\n    Args:\n        graph (Graph): A mercury graph structure.\n\n    Returns:\n        (self): Fitted self (or raises an error)\n\n    \"\"\"\n    if self.mode == \"networkx\":\n        self._fit_networkx(graph)\n    else:\n        self._fit_spark(graph)\n\n    return self\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.Transition","title":"<code>mercury.graph.ml.Transition()</code>","text":"<p>               Bases: <code>BaseClass</code></p> <p>Create an interface class to manage the adjacency matrix of a directed graph as a transition matrix. This enables computing distributions of probabilities over the nodes after a given number of iterations.</p> Source code in <code>mercury/graph/ml/transition.py</code> <pre><code>def __init__(self):\n    self.fitted_graph_ = None\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.Transition.fit","title":"<code>fit(G)</code>","text":"<p>Converts the adjacency matrix into a transition matrix. Transition matrices are used to compute the distribution of probability of being in each of the nodes (or states) of a directed graph (or Markov process). The distribution for state s is:</p> <ul> <li>\\(s_t = T*s_{t-1}\\)</li> </ul> <p>Where:</p> <p>T is the transition matrix. After calling.fit(), the adjacency matrix is the transition matrix. You can use .to_pandas() to see it. \\(s_{t-1}\\) is the previous state.</p> <p>What .fit() does is scaling the non-zero rows to make them sum 1 as they are probability distributions and make the zero rows recurrent states. A recurrent state is a final state, a state whose next state is itself.</p> <p>Parameters:</p> Name Type Description Default <code>G</code> <code>Graph</code> <p>A <code>mercury.graph</code> Graph.</p> required <p>Returns:</p> Type Description <code>self</code> <p>Fitted self (or raises an error).</p> Note <p>If created using NetworkX directly, the name of the weight must be 'weight' and must be positive. The recommended way to create the graph is using .set_row() which will always name the weight as 'weight' but does not check the value.</p> Source code in <code>mercury/graph/ml/transition.py</code> <pre><code>def fit(self, G: Graph):\n    \"\"\"\n    Converts the adjacency matrix into a transition matrix. Transition matrices are used to compute the distribution of probability\n    of being in each of the nodes (or states) of a directed graph (or Markov process). The distribution for state s is:\n\n    * $s_t = T*s_{t-1}$\n\n    Where:\n\n    T is the transition matrix. After calling.fit(), the adjacency matrix is the transition matrix. You can use .to_pandas() to see it.\n    $s_{t-1}$ is the previous state.\n\n    What .fit() does is scaling the non-zero rows to make them sum 1 as they are probability distributions and make the zero rows\n    recurrent states. A recurrent state is a final state, a state whose next state is itself.\n\n    Args:\n        G (Graph): A `mercury.graph` Graph.\n\n    Returns:\n        (self): Fitted self (or raises an error).\n\n    Note:\n        If created using NetworkX directly, the name of the weight must be 'weight' and must be positive. The recommended way\n        to create the graph is using .set_row() which will always name the weight as 'weight' but does not check the value.\n\n    \"\"\"\n    names = list(G.networkx.nodes)\n    adj_m = nx.adjacency_matrix(G.networkx, weight=\"weight\", dtype=float)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        for i in range(adj_m.shape[0]):\n            row = adj_m[[i], :]\n            tot = row.sum()\n\n            if tot == 0:\n                row[0, i] = 1\n            else:\n                row = row / tot\n\n            adj_m[[i], :] = row\n\n    df = pd.DataFrame(adj_m.todense(), index=names, columns=names)\n    self.fitted_graph_ = Graph(nx.from_pandas_adjacency(df, create_using=nx.DiGraph))\n\n    return self\n</code></pre>"},{"location":"reference/ml/#mercury.graph.ml.Transition.to_pandas","title":"<code>to_pandas(num_iterations=1)</code>","text":"<p>Returns the adjacency (which is the transition matrix after <code>fit()</code> was called) for a given number of iterations as a pandas dataframe with labeled rows and columns.</p> <p>Parameters:</p> Name Type Description Default <code>num_iterations</code> <code>int</code> <p>If you want to compute the matrix for a different number of iterations, k, you can use this argument to raise the matrix to any non negative integer, since \\(s_{t+k} = T^k*s_t\\)</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The transition matrix for num_iterations.</p> Note <p>This method does not automatically call <code>fit()</code>. This allows inspecting the adjacency matrix as a pandas dataframe. The result of computing num_iterations will not make sense if <code>fit()</code> has not been called before <code>to_pandas()</code>.</p> Source code in <code>mercury/graph/ml/transition.py</code> <pre><code>def to_pandas(self, num_iterations=1):\n    \"\"\"\n    Returns the adjacency (which is the transition matrix after `fit()` was called) for a given number of iterations as a pandas\n    dataframe with labeled rows and columns.\n\n    Args:\n        num_iterations (int): If you want to compute the matrix for a different number of iterations, k, you can use this argument to\n            raise the matrix to any non negative integer, since $s_{t+k} = T^k*s_t$\n\n    Returns:\n        (pd.DataFrame): The transition matrix for num_iterations.\n\n    Note:\n        This method does not automatically call `fit()`. This allows inspecting the adjacency matrix as a pandas dataframe.\n        The result of computing num_iterations will not make sense if `fit()` has not been called before `to_pandas()`.\n\n    \"\"\"\n    if self.fitted_graph_ is None:\n        raise ValueError(\"Error: fit() must be called first.\")\n\n    names = list(self.fitted_graph_.networkx.nodes)\n    adj_m = nx.adjacency_matrix(self.fitted_graph_.networkx, weight=\"weight\").todense()\n\n    if num_iterations != 1:\n        adj_m = matrix_power(adj_m, num_iterations)\n\n    return pd.DataFrame(adj_m, index=names, columns=names)\n</code></pre>"},{"location":"reference/viz/","title":"mercury.graph.viz","text":""},{"location":"reference/viz/#mercury.graph.viz.Moebius","title":"<code>mercury.graph.viz.Moebius(G)</code>","text":"<p>               Bases: <code>MoebiusAnywidget</code></p> <p>Moebius class for visualizing graphs using an anywidget.</p> <p>Important: This class requires: anywidget, traitlets and IPython.display!! These packages are not mandatory for the rest of the library, since you may not be intereseted in visualizing graphs. If you want to use this class, you need to install these packages.</p> Usage <pre><code>from mercury.graph.viz import Moebius\n\nG = ... # A graph object\nmoebius = Moebius(G)\nmoebius.show()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>G</code> <code>Graph</code> <p>The graph to be visualized.</p> required Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def __init__(self, G):\n    if display is None or HTML is None:\n        raise ImportError('IPython is not installed')\n\n    self.G = G\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Add support for the [] operator.</p> Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def __getitem__(self, item):\n    \"\"\"\n    Add support for the [] operator.\n    \"\"\"\n\n    return self._get_adjacent_nodes_moebius(item)\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius.__str__","title":"<code>__str__()</code>","text":"<p>Convert the object via str()</p> Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def __str__(self):\n    \"\"\"\n    Convert the object via str()\n    \"\"\"\n\n    return 'Moebius(%s)' % str(self.G)\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius._get_adjacent_nodes_moebius","title":"<code>_get_adjacent_nodes_moebius(node_id, limit=20, depth=1)</code>","text":"<p>This is the callback function used to interact with the Moebius JS library. Each time you deploy a node in the graph, this function is called to get the adjacent nodes and edges.</p> <p>The expected return is a JSON string with the following format:</p> <p>json = {     'nodes': [         {'id' : ..., 'count' : ..., '_int_id' : ...},         {'id' : ..., 'count' : ..., '_int_id' : ...}     ],     'links': [         {'source' : ..., 'target' : ..., '_int_id' : ...},         {'source' : ..., 'target' : ..., '_int_id' : ...}     ] }, where 'count' is the degree of the given node, and '_int_id' is a unique integer identifier for the given node or edge.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The id of the node to get the adjacent nodes.</p> required <code>limit</code> <code>int</code> <p>The maximum number of nodes to be returned.</p> <code>20</code> <code>depth</code> <code>int</code> <p>The depth of the graph to be returned.</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string with the adjacent nodes and edges.</p> Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def _get_adjacent_nodes_moebius(self, node_id, limit = 20, depth = 1):\n    \"\"\"\n    This is the callback function used to interact with the Moebius JS library. Each time you deploy a node in the graph, this\n    function is called to get the adjacent nodes and edges.\n\n    The expected return is a JSON string with the following format:\n\n    json = {\n        'nodes': [\n            {'id' : ..., 'count' : ..., '_int_id' : ...},\n            {'id' : ..., 'count' : ..., '_int_id' : ...}\n        ],\n        'links': [\n            {'source' : ..., 'target' : ..., '_int_id' : ...},\n            {'source' : ..., 'target' : ..., '_int_id' : ...}\n        ]\n    },\n    where 'count' is the degree of the given node, and '_int_id' is a\n    unique integer identifier for the given node or edge.\n\n    Args:\n        node_id (str): The id of the node to get the adjacent nodes.\n        limit (int): The maximum number of nodes to be returned.\n        depth (int): The depth of the graph to be returned.\n\n    Returns:\n        (str): A JSON string with the adjacent nodes and edges.\n    \"\"\"\n\n    if self.use_spark:\n        nodes_df, edges_df = self._get_one_level_subgraph_graphframes(node_id)\n        N = nodes_df.count()\n\n    else:\n        nodes_df, edges_df = self._get_one_level_subgraph_networkx(node_id)\n        N = len(nodes_df)\n\n    d = 1\n    expanded = set([node_id])\n\n    while N &lt; limit and d &lt; depth:\n        if self.use_spark:\n            next = set(nodes_df.select('id').rdd.flatMap(lambda x: x).collect())\n            for id in next:\n                if id not in expanded:\n                    expanded.add(id)\n\n                    next_nodes, next_edges = self._get_one_level_subgraph_graphframes(id)\n                    nodes_df = nodes_df.union(next_nodes).distinct()\n                    edges_df = edges_df.union(next_edges).distinct()\n\n                    N = nodes_df.count()\n                    if N &gt;= limit:\n                        break\n        else:\n            next = set(nodes_df.id)\n            for id in next:\n                if id not in expanded:\n                    expanded.add(id)\n\n                    next_nodes, next_edges = self._get_one_level_subgraph_networkx(id)\n                    nodes_df = pd.concat([nodes_df, next_nodes]).drop_duplicates().reset_index(drop = True)\n                    edges_df = pd.concat([edges_df, next_edges]).drop_duplicates().reset_index(drop = True)\n\n                    N = len(nodes_df)\n                    if N &gt;= limit:\n                        break\n\n        d += 1\n\n    if self.use_spark:\n        json_final = {\n            'nodes': json.loads(nodes_df.toPandas().to_json(orient = 'records', force_ascii = False)),\n            'links': json.loads(edges_df.toPandas().to_json(orient = 'records', force_ascii = False))\n        }\n\n    else:\n        json_final = {\n            'nodes': json.loads(nodes_df.to_json(orient = 'records', force_ascii = False)),\n            'links': json.loads(edges_df.to_json(orient = 'records', force_ascii = False))\n        }\n\n    return json.dumps(json_final, ensure_ascii = False)\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius._get_one_level_subgraph_graphframes","title":"<code>_get_one_level_subgraph_graphframes(node_id, _testing=False)</code>","text":"<p>Get the one-level subgraph for a given node ID using GraphFrames.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node for which to get the one-level subgraph.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing two Spark DataFrames: - nodes_df: DataFrame with columns 'id', 'count', '_int_id', and any other node attributes. - edges_df: DataFrame with columns 'source', 'target', '_int_id' the edges connecting the nodes in the subgraph.</p> Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def _get_one_level_subgraph_graphframes(self, node_id, _testing=False):\n    \"\"\"\n    Get the one-level subgraph for a given node ID using GraphFrames.\n\n    Args:\n        node_id (str): The ID of the node for which to get the one-level subgraph.\n\n    Returns:\n        (tuple): A tuple containing two Spark DataFrames:\n            - nodes_df: DataFrame with columns 'id', 'count', '_int_id', and any other node attributes.\n            - edges_df: DataFrame with columns 'source', 'target', '_int_id' the edges connecting the nodes in the subgraph.\n    \"\"\"\n\n    sql      = SparkInterface().pyspark.sql\n    spark    = SparkInterface().spark\n    F        = sql.functions\n    LongType = sql.types.LongType\n\n    graph = self.G.graphframe\n\n    edges_df = graph.edges.filter((graph.edges.src == node_id) | (graph.edges.dst == node_id))\n\n    N = len(self._int_id_map)\n    int_id_map_broadcast = spark.sparkContext.broadcast(self._int_id_map)\n\n    def edge_int_id(src, dst):\n        int_id_map = int_id_map_broadcast.value\n        return int_id_map[src] + N*(int_id_map[dst] + 1)\n\n    edge_int_id_udf = F.udf(edge_int_id, LongType())\n\n    edges_df = edges_df.withColumn('_int_id', edge_int_id_udf(F.col('src'), F.col('dst')))\n    edges_df = edges_df.withColumnRenamed('src', 'source').withColumnRenamed('dst', 'target')\n\n    order = ['source', 'target', '_int_id']\n    for col in edges_df.columns:\n        if col not in order:\n            order.append(col)\n    edges_df = edges_df.select(order)\n\n    node_ids = edges_df.select('source').union(edges_df.select('target')).distinct()\n    nodes_df = node_ids.join(graph.vertices, node_ids.source == graph.vertices.id, 'inner').select(graph.vertices['*'])\n\n    degrees  = graph.degrees\n    # Edges are duplicated in undirected graphframes -&gt; Fix degree\n    if not self.G.is_directed:\n        degrees = degrees.withColumn('degree', F.ceil(F.col('degree') / 2))\n    nodes_df = nodes_df.join(degrees, on = 'id', how = 'left').withColumnRenamed('degree', 'count')\n\n    def node_int_id(id):\n        int_id_map = int_id_map_broadcast.value\n        return int_id_map[id]\n\n    if _testing:\n        key, val = next(iter(self._int_id_map.items()))\n        assert node_int_id(key) == val\n        assert edge_int_id(key, key) == N*val + N + val\n\n    node_int_id_udf = F.udf(node_int_id, LongType())\n\n    nodes_df = nodes_df.withColumn('_int_id', node_int_id_udf(F.col('id')))\n\n    order = ['id', 'count', '_int_id']\n    for col in nodes_df.columns:\n        if col not in order:\n            order.append(col)\n    nodes_df = nodes_df.select(order)\n\n    return nodes_df, edges_df\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius._get_one_level_subgraph_networkx","title":"<code>_get_one_level_subgraph_networkx(node_id)</code>","text":"<p>Get the one-level subgraph for a given node ID using Networkx.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>The ID of the node for which to get the one-level subgraph.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing two Pandas DataFrames: - nodes_df: DataFrame with columns 'id', 'count', '_int_id', and any other node attributes. - edges_df: DataFrame with columns 'source', 'target', '_int_id' the edges connecting the nodes in the subgraph.</p> Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def _get_one_level_subgraph_networkx(self, node_id):\n    \"\"\"\n    Get the one-level subgraph for a given node ID using Networkx.\n\n    Args:\n        node_id (str): The ID of the node for which to get the one-level subgraph.\n\n    Returns:\n        (tuple): A tuple containing two Pandas DataFrames:\n            - nodes_df: DataFrame with columns 'id', 'count', '_int_id', and any other node attributes.\n            - edges_df: DataFrame with columns 'source', 'target', '_int_id' the edges connecting the nodes in the subgraph.\n    \"\"\"\n\n    graph = self.G.networkx\n\n    if self.G.is_directed:\n        neighbors = set(graph.neighbors(node_id)) | set(graph.predecessors(node_id))\n    else:\n        # Undirected graphs are symmetric: predecessors == successors (or neighbors)\n        neighbors = set(graph.neighbors(node_id))\n\n    neighbors.add(node_id)\n\n    subgraph = graph.subgraph(neighbors)\n\n    # Create nodes DataFrame\n    nodes_data = []\n    for node in subgraph.nodes(data = True):\n        node_id = node[0]\n        attributes = node[1].copy()\n        attributes['id'] = node_id\n        attributes['count'] = graph.degree[node_id]\n        attributes['_int_id'] = self._int_id_map[node_id]\n        nodes_data.append(attributes)\n    nodes_df = pd.DataFrame(nodes_data)\n\n    order = ['id', 'count', '_int_id']\n    for col in nodes_df.columns:\n        if col not in order:\n            order.append(col)\n    nodes_df = nodes_df[order]\n\n    # Create edges DataFrame\n    edges_data = []\n    N = len(self._int_id_map)\n    for edge in subgraph.edges(data = True):\n        src, dst, attributes = edge\n        attributes = attributes.copy()\n        attributes['source'] = src\n        attributes['target'] = dst\n        attributes['_int_id'] = self._int_id_map[src] + N*(self._int_id_map[dst] + 1)\n        edges_data.append(attributes)\n    edges_df = pd.DataFrame(edges_data)\n\n    order = ['source', 'target', '_int_id']\n    for col in edges_df.columns:\n        if col not in order:\n            order.append(col)\n    edges_df = edges_df[order]\n\n    return nodes_df, edges_df\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius._hsl_to_rgb","title":"<code>_hsl_to_rgb(h, s, l)</code>","text":"<p>Convert HSL (Hue, Saturation, Lightness) color space to RGB (Red, Green, Blue) color space.</p> <p>Parameters: h (float): Hue value, should be between 0 and 1. s (float): Saturation value, should be between 0 and 1. l (float): Lightness value, should be between 0 and 1.</p> <p>Returns: tuple: A tuple containing the RGB values (r, g, b), each ranging from 0 to 255.</p> Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def _hsl_to_rgb(self, h, s, l):\n    \"\"\"\n    Convert HSL (Hue, Saturation, Lightness) color space to RGB (Red, Green, Blue) color space.\n\n    Parameters:\n    h (float): Hue value, should be between 0 and 1.\n    s (float): Saturation value, should be between 0 and 1.\n    l (float): Lightness value, should be between 0 and 1.\n\n    Returns:\n    tuple: A tuple containing the RGB values (r, g, b), each ranging from 0 to 255.\n    \"\"\"\n    def hue_to_rgb(p, q, t):\n        if t &lt; 0: t += 1\n        if t &gt; 1: t -= 1\n        if t &lt; 1/6: return p + (q - p)*6*t\n        if t &lt; 1/2: return q\n        if t &lt; 2/3: return p + (q - p)*(2/3 - t)*6\n        return p\n\n    q = l + s - l*s if l &lt; 0.5 else l + s - l*s\n    p = 2*l - q\n    r = hue_to_rgb(p, q, h + 1/3)\n    g = hue_to_rgb(p, q, h)\n    b = hue_to_rgb(p, q, h - 1/3)\n\n    return int(255*r), int(255*g), int(255*b)\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius._pd_to_json_format","title":"<code>_pd_to_json_format(df)</code>","text":"<p>A utility to produced the flavor of JSON expected by the JS library from a pandas DataFrame.</p> Usage <pre><code># This simple snippet to convert the whole graph to the expected JSON format.\n\nedges = self._pd_to_json_format(self.G.edges_as_pandas().rename({'src' : 'source', 'dst' : 'target'}, axis = 1))\nnodes = self._pd_to_json_format(self.G.nodes_as_pandas())\n\njson_final = {'nodes' : nodes, 'links' : edges}\n\nreturn json.dumps(json_final, ensure_ascii = False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to be converted to JSON.</p> required Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def _pd_to_json_format(self, df):\n    \"\"\"\n    A utility to produced the flavor of JSON expected by the JS library from a pandas DataFrame.\n\n    Usage:\n        ```python\n        # This simple snippet to convert the whole graph to the expected JSON format.\n\n        edges = self._pd_to_json_format(self.G.edges_as_pandas().rename({'src' : 'source', 'dst' : 'target'}, axis = 1))\n        nodes = self._pd_to_json_format(self.G.nodes_as_pandas())\n\n        json_final = {'nodes' : nodes, 'links' : edges}\n\n        return json.dumps(json_final, ensure_ascii = False)\n        ```\n\n    Args:\n        df (pd.DataFrame): The DataFrame to be converted to JSON.\n    \"\"\"\n\n    df = df.reset_index(drop = True)\n\n    df_json = df.to_json(orient = 'records', force_ascii = True, date_format = 'iso')\n\n    return json.loads(df_json)\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius._repr_mimebundle_","title":"<code>_repr_mimebundle_(include=None, exclude=None)</code>","text":"<p>Override display() method to control its execution and avoid errors.</p> Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def _repr_mimebundle_(self, include=None, exclude=None):\n    \"\"\"\n    Override display() method to control its execution and avoid errors.\n    \"\"\"\n    self.show()\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius.generate_color_palette","title":"<code>generate_color_palette(cats, hue=0, sat=0.7, light=0.5)</code>","text":"<p>Generates a color palette for the given categories. This can be used in combination with <code>node_or_edge_config</code> to generate the dictionary expected by the <code>colors</code> argument with colors that cover the whole 0..1 hue range.</p> <p>Parameters:</p> Name Type Description Default <code>cats</code> <code>iterable</code> <p>An iterable of categories for which the color palette is to be generated.</p> required <code>hue</code> <code>float</code> <p>The base hue that is added to all the colors in the color palette. It must be in range, 0..1, all the resulting hue values will be kept modulo 1.0. Default is 0 (no shift).</p> <code>0</code> <code>sat</code> <code>float</code> <p>The saturation level for the colors. Default is 0.7. Range is 0..1.</p> <code>0.7</code> <code>light</code> <code>float</code> <p>The lightness level for the colors. Default is 0.5. Range is 0..1.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where keys are categories and values are hex color codes.</p> Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def generate_color_palette(self, cats, hue = 0, sat = 0.7, light = 0.5):\n    \"\"\"\n    Generates a color palette for the given categories. This can be used in combination with `node_or_edge_config` to generate\n    the dictionary expected by the `colors` argument with colors that cover the whole 0..1 hue range.\n\n    Args:\n        cats (iterable): An iterable of categories for which the color palette is to be generated.\n        hue (float, optional): The base hue that is added to all the colors in the color palette. It must be in range, 0..1, all the\n            resulting hue values will be kept modulo 1.0. Default is 0 (no shift).\n        sat (float, optional): The saturation level for the colors. Default is 0.7. Range is 0..1.\n        light (float, optional): The lightness level for the colors. Default is 0.5. Range is 0..1.\n\n    Returns:\n        (dict): A dictionary where keys are categories and values are hex color codes.\n    \"\"\"\n    cats = set(cats)\n    cols = {}\n    N = len(cats)\n    for i, cat in enumerate(cats):\n        h = (i/N + hue) % 1.0\n        s = sat\n        l = light\n\n        r, g, b = self._hsl_to_rgb(h, s, l)\n\n        hex_color = '#%02x%02X%02x' % (r, g, b)\n        cols[cat] = hex_color\n\n    return cols\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius.node_or_edge_config","title":"<code>node_or_edge_config(text_is=None, color_is=None, colors=None, size_is=None, size_range=None, size_scale='linear')</code>","text":"<p>Create a <code>node_config</code> or <code>edge_config</code> configuration dictionary for <code>show()</code> in an understandable way.</p> <p>Parameters:</p> Name Type Description Default <code>text_is</code> <code>str</code> <p>The node/edge attribute to be displayed as text. Use the string <code>\u00ecd</code> to draw the node id (regardless of the column having another name) or any valid node attribute name.</p> <code>None</code> <code>color_is</code> <code>str</code> <p>A categorical node/edge attribute that can be represented as a color. This will also enable a legend interface where categories can be individually shown or hidden.</p> <code>None</code> <code>colors</code> <code>dict</code> <p>The colors for each category defined as a dictionary. The keys are possible outcomes of category. The values are html RGB strings. E.g., .draw(category = 'size', colors = {'big' : '#c0a080', 'small' : '#a0c080'}) where 'big' and 'small' are possible values of the category 'size'.</p> <code>None</code> <code>size_is</code> <code>str</code> <p>The node attribute to be displayed as the size of the nodes. Use the string <code>id</code> to set the node id (regardless of the column having another name) or any valid node attribute name. See the options in the Moebius configuration menu to set minimum, maximum sizes, linear or logarithmic scale, etc.</p> <code>None</code> <code>size_range</code> <code>List of two numbers</code> <p>Combined with edge_label, this parameter controls the values in the variable that correspond to the minimum and maximum displayed sizes. The values below or equal the first value will be displayed with the base radius (that depends on the zoom) and the values above or equal to the second value will be shown with the maximum radius.</p> <code>None</code> <code>size_scale</code> <code>(linear, power, sqrt or log)</code> <p>Combined with edge_label, the scale used to convert the value in the variable to the displayed radius.</p> <code>'linear'</code> <p>Returns:</p> Type Description <code>dict</code> <p>The node configuration dictionary</p> Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def node_or_edge_config(self, text_is = None, color_is = None, colors = None, size_is = None, size_range = None, size_scale = 'linear'):\n    \"\"\"\n    Create a `node_config` or `edge_config` configuration dictionary for `show()` in an understandable way.\n\n    Args:\n        text_is (str): The node/edge attribute to be displayed as text. Use the string `\u00ecd` to draw the node id (regardless of the\n            column having another name) or any valid node attribute name.\n        color_is (str): A categorical node/edge attribute that can be represented as a color. This will also enable a legend interface\n            where categories can be individually shown or hidden.\n        colors (dict): The colors for each category defined as a dictionary. The keys are possible outcomes of category.\n            The values are html RGB strings. E.g., .draw(category = 'size', colors = {'big' : '#c0a080', 'small' : '#a0c080'})\n            where 'big' and 'small' are possible values of the category 'size'.\n        size_is (str): The node attribute to be displayed as the size of the nodes. Use the string `id` to set the node id (regardless\n            of the column having another name) or any valid node attribute name. See the options in the Moebius configuration menu to\n            set minimum, maximum sizes, linear or logarithmic scale, etc.\n        size_range (List of two numbers): Combined with edge_label, this parameter controls the values in the variable that\n            correspond to the minimum and maximum displayed sizes. The values below or equal the first value will be displayed with the\n            base radius (that depends on the zoom) and the values above or equal to the second value will be shown with the maximum\n            radius.\n        size_scale ('linear', 'power', 'sqrt' or 'log'): Combined with edge_label, the scale used to convert the value in the variable\n            to the displayed radius.\n\n    Returns:\n        (dict): The node configuration dictionary\n    \"\"\"\n\n    config = {}\n\n    if text_is is not None:\n        config['label'] = text_is\n\n    if color_is is not None:\n        config['color'] = color_is\n\n    if colors is not None:\n        config['color_palette'] = colors\n    else:\n        config['color_palette'] = {}\n\n    if size_is is None:\n        config['size_thresholds'] = []\n    else:\n        config['size'] = size_is\n\n        if size_range is None:\n            config['size_thresholds'] = []\n        else:\n            assert type(size_range) == list and len(size_range) == 2\n            config['size_thresholds'] = size_range\n\n        if size_scale != 'linear':\n            assert size_scale in {'power', 'sqrt', 'log'}\n\n        config['scale'] = size_scale\n\n    return config\n</code></pre>"},{"location":"reference/viz/#mercury.graph.viz.Moebius.show","title":"<code>show(initial_id=None, initial_depth=1, node_config=None, edge_config=None)</code>","text":"<p>Start the interactive graph visualization using an anywidget.</p> <p>Parameters:</p> Name Type Description Default <code>initial_id</code> <code>str</code> <p>The id of the node to start the visualization.</p> <code>None</code> <code>initial_depth</code> <code>int</code> <p>The initial depth of the graph (starting with <code>initial_id</code> as 0) to be shown.</p> <code>1</code> <code>node_config</code> <code>dict</code> <p>A node configuration dictionary created by <code>node_config()</code>.</p> <code>None</code> <code>edge_config</code> <code>dict</code> <p>An edge configuration dictionary created by <code>edge_config()</code>.</p> <code>None</code> Source code in <code>mercury/graph/viz/moebius.py</code> <pre><code>def show(self, initial_id = None, initial_depth = 1, node_config = None, edge_config = None):\n    \"\"\"\n    Start the interactive graph visualization using an anywidget.\n\n    Args:\n        initial_id (str): The id of the node to start the visualization.\n        initial_depth (int): The initial depth of the graph (starting with `initial_id` as 0) to be shown.\n        node_config (dict): A node configuration dictionary created by `node_config()`.\n        edge_config (dict): An edge configuration dictionary created by `edge_config()`.\n    \"\"\"\n    display(MoebiusAnywidget(self.G, initial_id, initial_depth, node_config, edge_config))\n</code></pre>"}]}